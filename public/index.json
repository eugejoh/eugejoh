[{"authors":["admin"],"categories":null,"content":"Eugene is an epidemiologist with an interdisciplinary background and a strong interest in leveraging disparate data sources to solve public health problems.\nMy main area of work is centered around infectious disease epidemiology, but I have parallel interests in reproducible workflows, open source tools, and the intersection of data science with public health research.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1585164215,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Eugene is an epidemiologist with an interdisciplinary background and a strong interest in leveraging disparate data sources to solve public health problems.\nMy main area of work is centered around infectious disease epidemiology, but I have parallel interests in reproducible workflows, open source tools, and the intersection of data science with public health research.","tags":null,"title":"Eugene Joh","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1580687852,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580687852,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580687852,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":"","date":1582848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585164281,"objectID":"90b3d5f42218269288225904b721f296","permalink":"/talk/test/","publishdate":"2020-02-28T00:00:00Z","relpermalink":"/talk/test/","section":"talk","summary":"Cleaning messy text to join data in a global health application","tags":[],"title":"Cleaning Data and Fuzzy Matching in R","type":"talk"},{"authors":null,"categories":null,"content":"Send Telegram Notifications with R The sendtg package is provides a number of simple R functions to send Telegram messages. This is a wrapper around the telegram.bot R package.\nThe link to the GitHub repository is above or here .\n","date":1581206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581294383,"objectID":"54c61f3d936163f07967c921e9d2baaf","permalink":"/project/sendtg/","publishdate":"2020-02-09T00:00:00Z","relpermalink":"/project/sendtg/","section":"project","summary":"`sendtg` is a wrapper R package that simplifies sending Telegram messages from R","tags":["R","Databases"],"title":"Send Telegram messages from R","type":"project"},{"authors":null,"categories":null,"content":"WHO Mortality Database The WHOmortality R package provides tools to easily download, extract, and import the raw data files found in the World Health Organization\u0026rsquo;s (WHO) Mortality Database.\nCurrently this package only imports the two mortality tables containing the International Classification of Disease, 10th Revision (ICD-10) codes.\nMore information can be found in the GitHub repository README .\n","date":1581206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585790720,"objectID":"85e2e33727e78e933fe344cd3be7dc2e","permalink":"/project/whomortality/","publishdate":"2020-02-09T00:00:00Z","relpermalink":"/project/whomortality/","section":"project","summary":"`WHOmortality` is a R package imports data from the World Health Organization's Mortality Database","tags":["R","Public Health","Databases"],"title":"WHO Mortality Database","type":"project"},{"authors":null,"categories":null,"content":"Tables from R to PostgreSQL pgtools Is an R package that contains functions to provide a consistent workflow for writing a data.frame in R as a PostgreSQL database table. The tools are built around the DBI and RPostgres packages.\nWhat this package provides?\n Arguments to specify table and variable comments in an efficient manner Conversion of R column types to PostgreSQL field types Schema specification for writing Simple genration of a SQL CREATE TABLE ... statements Convenient connection to PostgreSQL with credentials based on /.Renviron Vectorized to accept a list of data.frames to write to PostgreSQL  This was created for primary use among the data management team at the Centre for Global Health Research to standardize and optimize the data storage workflow.\n Link to the GitHub repository.\n","date":1581206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581293321,"objectID":"25dd545a998afcea48a885badc32ff3c","permalink":"/project/pgtools/","publishdate":"2020-02-09T00:00:00Z","relpermalink":"/project/pgtools/","section":"project","summary":"`pgtools` is an R package with tools to write data frames as PostgreSQL tables","tags":["R","Databases"],"title":"Writing tables from R to PostgreSQL","type":"project"},{"authors":["Eugene Joh"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math .\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580687852,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":null,"categories":["R","Hemodynamics"],"content":" Original post date: April 05 2018\nThis post isn’t public health related (like this one) but they will be more of a personal exploration and application of different tools and packages in R. This post will be exploring the animation and gganimate package to create some (hopefully) cool animated visualizations.\nLaminar Flow A simple definition of laminar flow is when a fluid flows through a pipe in parallel layers with no disruptions between these layers. Turbulent flow occurs when there is mixing or any disruption between these layers. Laminar flow is a simple analysis of fluid dynamics that considers the viscosity (“thickness/gooeyness”) of a liquid. This model is based on a number of assumptions where the fluid flows through a straight cylindrical pipe of fixed diameter, there is no acceleration (only a balance between pressure and viscous/shear forces) and gravitational forces are ignored. The walls of the pipe exhibit the maximum shearing force and the axial centre of the pipe has zero shear. This results in a parabolic velocity profile where the velocity is at it’s maximum at centre of the pipe.\n MATLAB to R? First you might wonder, why is this post randomly on laminar flow? A short trip down memory lane while organizing some undergraduate course folders resulted in my rediscovery of some lecture notes and code related to medical biophysics and hemodynamics. Not having used MATLAB in over 4 years, I was curious whether I could translate some of the MATLAB code into something similar into R. Some points on the differences, MATLAB is a proprietary numerical computing programming software based on matrix algorithms while R is a open-source programming language centered on statistical analyses. There are some StackOverflow discussions on these comparisons if you want to explore this further. I also discovered the pracma package for numerical analysis that uses some MATLAB function names (#tbt). Now to the actual R code!\n Initial Conditions First we’ll load the relevant packages. I’ve commented in the code lines on some basic descriptions on each package. Just a small note that some of these packages need to be downloaded from GitHub repositories using the devtools package.\n# Packages #### library(pracma) #load practical math package library(ggplot2) #data viz library(scales) #plot scales library(dplyr) #data wrangling library(viridis) #data viz palette library(animation) #animation library(gganimate) #ggplot animation Next, for any type of model you need to first set the initial conditions or parameters. In this case, we’ll specify the conditions for the pressure difference at the beginning at the end of the pipe, the viscous force, the dimensions of the pipe (radius and length).\n## Initial Conditions #### # Pressure difference (delta P) Pi \u0026lt;- 10 #initial pressure [Pa] Pf \u0026lt;- 50 #final pressure [Pa] ## Boundary Conditions #### # Pipe and fluid characteristics mu \u0026lt;- 8.94e-4 #viscosity for water. units: [Pa*s] at temp = 25 C R \u0026lt;- 0.1 #radius of pipe [m] l \u0026lt;- 4 #length of pipe [m] N \u0026lt;- 100 #radial resolution r \u0026lt;- seq(-R,R,length.out=N) #diameter of pipe S \u0026lt;- pi*(R^2) #cross-sectional area of pipe Since we’re also playing with the animation package, we’ll also input a vector of time units to loop and animate the visualization.\n# time dependent time \u0026lt;- seq(0,19) #length 20 Pit \u0026lt;- 0.2*time*Pi Pft \u0026lt;- 0.2*time*Pf Pdiff \u0026lt;- abs(Pft-Pit) #pressure difference over time  Data Setup Now what we’ll do is run a loop over the time sequence we’ve specified (20 arbitrary units of time, seconds for example) to create the output data. The pressure difference will be increased as a function of time, simulating increased flow of the fluid (water in this case) over time. The equation in the code below is the flow velocity as a function of the pipe radius derived from Hagen-Poiseuille’s Law specified below. So after running lapply() over the pressure differences, we then massage the list output into a data frame – first by “unlisting” the list elements, creating a matrix object where each column represents a different time point, and coercing the matrix into a data frame (and as always remembering to use stringsAsFactors = FALSE).\n Hagen-Poiseuille’s Law\nQ = πR4Δp / 8μl\n # Create Data Viz #### # plot shows V (average speed) and r (pipe radius) # ((Pdiff)*(R^2)/(4*mu*l))*(1-(r^2)/R^2) #eqn for V out1 \u0026lt;- lapply(seq_along(Pdiff), function(x) { ((Pdiff[x]) * (R ^ 2) / (4 * mu * l)) * (1 - (r ^ 2) / R ^ 2) }) out1_df \u0026lt;-data.frame( matrix(unlist(out1), ncol = length(time)), stringsAsFactors = FALSE ) names(out1_df) \u0026lt;- seq_along(names(out1_df)) Now we have a data frame with each column containing the velocity of the fluid as a function of the radius. There are twenty time periods and 100 rows representing the resolution of the x-axis (e.g. 100 ticks). The problem now is that this data frame is not in a ggplot2 friendly format. We’ll have to use the tidyr package to manipulate the data. The code below “gathers” the columns so they become grouped elements in one column and then we use the dplyr package to add the radius values as new column, coerce the “time” column into an integer type, and add the flowrate (a function of the velocity profile and pipe cross-sectional area) as another column. A note here that when the rbind() function is used, if the length/# of rows of the object’s (r) being “binded”\u0026quot; is a multiple of the other object’s (df_t) length/# of rows, it will repeat based on the multiple. Since we looped based on the the radial resolution (100) specified before, rbind() will fill the column with 20 multiples (20*100) of the radius ticks.\ndf_t \u0026lt;- tidyr::gather(out1_df,key=\u0026quot;time\u0026quot;,value=\u0026quot;V\u0026quot;) df_t2 \u0026lt;- df_t %\u0026gt;% #previous object mutate(time = as.integer(time)) %\u0026gt;% #change to integer type mutate(Qv = V/(1-(r^2)/R^2)) #add volumetric flowrate (Qv)  Data Visualization Now we have our data in a nice format to visualize and loop over to create our GIF. We specify the data frame we want to use, map the aesthetics and specifically the frame and cumulative arguments. The frame aesthetic tells the gganimate package to loop over the values defined in frame or time in our case. We’ll use the amazing viridis package to add some time colours, use the built-in ggplot2 theme_dark, tinker with the text sizes, and provide a good title and axis labels (yay for plotting best practices). The legend here displays the flow rate through the pipe going from purple (low flowrate) to yellow (high flowrate).\np1 \u0026lt;- ggplot(data=df_t2, aes(y=r,x=V,col=Qv,frame=time,cumulative=TRUE)) + geom_path() + geom_point(alpha=0.05) + scale_colour_viridis(option = \u0026quot;C\u0026quot;, discrete = FALSE) + theme_dark() + labs(title = \u0026quot;Hagen-Poiseuille\u0026#39;s Law: Velocity Profile t =\u0026quot;, y = \u0026quot;Pipe Radius (m)\u0026quot;, x = \u0026quot;Velocity (m/s)\u0026quot;) ggsave(p1,filename = \u0026quot;Vprofile1.png\u0026quot;,width = 250, height=100,units = \u0026quot;mm\u0026quot;)  The title might look strange but the empty “t =”\u0026quot; will be useful to indicate which time point each frame describes when we create the .gif (look at the title_frame argument in the gganimate() function below. This will add the value provided to the frame aesthetic and include it into the plot title. We also specify the intervals for each frame (0.2 seconds) and the dimensions of the output.\ngganimate(p1,filename = \u0026quot;Vprofile1.gif\u0026quot;, title_frame = TRUE, interval=0.2, ani.width = 900, ani.height = 350)  Now we have this nice animation visualizing the laminar flow velocity profile of a pipe at different time points where the pressure difference increases, in turn increasing the maximum velocity of the fluid. Another way to see it, is that at time 0, there is no pressure difference, therefore no flow. But once the pressure difference increases (decreased lower pressure at the end of pipe e.g. increased flow of fluid out of the end of the pipe) the velocity profile or flow increases.\nThe entire script containing the code is found in my GitHub repo if you would want to fork it or tinker with it yourself.\n ","date":1522886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586625311,"objectID":"b870f587b570508ba2726f664e75401b","permalink":"/post/laminar-flow/","publishdate":"2018-04-05T00:00:00Z","relpermalink":"/post/laminar-flow/","section":"post","summary":"Original post date: April 05 2018\nThis post isn’t public health related (like this one) but they will be more of a personal exploration and application of different tools and packages in R. This post will be exploring the animation and gganimate package to create some (hopefully) cool animated visualizations.\nLaminar Flow A simple definition of laminar flow is when a fluid flows through a pipe in parallel layers with no disruptions between these layers.","tags":[],"title":"Laminar flow with ggplot2 and gganimate","type":"post"},{"authors":null,"categories":["R","Water Quality"],"content":" Original post date: December 09 2017\nThis post goes into using open data provided by the City of Toronto and all the code is found on Github.\nDrinking Water Water is a basic physiological need for humans. We can go days without food and be okay, but without water… that’s another story. Drinking water or potable water is something in the developed world we take for granted. Much of the world does not have great access to clean water and by United Nations had made their 6th Sustainable Development Goal to “ensure access to water and sanitation for all” and that all people would have universal access to safe and affordable water by 2030.\nIn Ontario (province of Canada) we are fortunate to have good water quality in most cities, towns, municipalities. It’s important to note that a disparity exists between regions across the province: large vs. small cities, southern vs. northern locations, and within indigenous communities. Across the province, there are different types of drinking water systems, various combinations of owners/operators, and (complex) regulatory network ensuring that the quality of the water reaching users is acceptable for consumption.\n Watermain Breaks Within these drinking water systems, the large pipes that carry most of the water through the system from the treatment plant through the distribution system are called watermains (or water mains). In the event of a watermain failing, either by a crack, some leaking or a catastrophic break, this can have potential effects on the flow of water to users. There can be loss of pressure in the distribution system, potentially allowing contaminants (chemical or microbiological) to enter the drinking water system due to the loss of pressure. No one wants to have E. coli or some industrial chemical in their tap water.\nThere are many factors behind a watermain failure. Most of the time there multiple concurrent factors that cause the pipe to crack or break. A non-exhaustive list of factors include type of pipe material, age of pipe, surrounding soil conditions, corrosion, pressure fluctuations, direct damage, extreme temperatures, extreme weather events, and poor system design.\n  Toronto Watermain Break Data The City of Toronto has an Open Data Catalogue containing various types of datasets on a number of topics like environment, public safety, development, and government. You can follow them on Twitter to know when they update/refresh their data. I downloaded the Excel flat-file containing the date and geographic location of watermain breaks from 1990 to 2016 within the city of Toronto boundaries. For this post I will be focusing on the temporal nature of the data, aka what do the trends of watermain breaks look like over time in Toronto.\n Data Import, Exploration, Processing After downloading the Excel file into my data folder, I use the list.files() function and some regular expression to identify the .xlsx files in the folder. This isn’t necessary since we only have one file in our folder but I’ve decided to make it good practice to have a standard method in selecting multiple files and file types in my data folder in a programmatic fashion. Afterwards I use the read_excel() function from the readxl package to import the dataset into R.\nwatermain.files \u0026lt;- list.files(\u0026quot;data\u0026quot;, pattern = \u0026quot;\\\\.xlsx$\u0026quot;) wm.df \u0026lt;- readxl::read_excel(paste0(\u0026quot;data/\u0026quot;,watermain.files)) We see that there are 35461 rows or (assumed) unique watermain breaks. After using the str() function to get a basic idea of the types of columns I have. I rename the column names using the names() function and create a new column with the year as a factor type (I actually don’t end up using this).\nwm.df \u0026lt;- read_excel(paste0(\u0026quot;data/\u0026quot;,watermain.files)) names(wm.df) \u0026lt;- c(\u0026quot;Date\u0026quot;,\u0026quot;Year\u0026quot;,\u0026quot;X_coord\u0026quot;,\u0026quot;Y_coord\u0026quot;) #change column names wm.df$Year_f \u0026lt;- as.factor(wm.df$Year) #create new column with \u0026#39;Year\u0026#39; as factor wm.df$Year \u0026lt;- as.integer(wm.df$Year) #convert original \u0026#39;Year\u0026#39; to an integer Next I use the floor_date() function from the lubridate package to create new columns containing the month and week. This will be useful later on when I want to aggregate the frequency counts of watermain breaks by week or month.\nwm.df$week \u0026lt;- floor_date(wm.df$Date, unit = \u0026quot;week\u0026quot;) #floor to week wm.df$month \u0026lt;- floor_date(wm.df$Date, unit = \u0026quot;month\u0026quot;) #floor to month Then I just create a new column that contains the same date information but instead of POSIXct, it is in Date format.\nwm.df \u0026lt;- wm.df %\u0026gt;% mutate(date = as.Date(Date)) This is not related to the temporal information but a good step with the data exploratory workflow. I want to do a bit a quality control/assurance with the data I have. I use the summary() function to look at the Date, Year, X_coord, and Y_coord columns. First thing that catches my eye is that there seems to be some extreme values or outliers in the spatial information, specifically the X_coord maximum and Y_coord minimum. For geographic information, typical errors include inputting latitude when it should be longitude, and vice-versa.\nwm.df %\u0026gt;% select(X_coord, Y_coord) %\u0026gt;% summary() # X_coord Y_coord # Min. : 294164 Min. : 304472 # 1st Qu.: 303580 1st Qu.:4838095 # Median : 311195 Median :4842843 # Mean : 311820 Mean :4841655 # 3rd Qu.: 318587 3rd Qu.:4846416 # Max. :4845682 Max. :4855952 First I look at the X_coord first and discover that the maximum value is not likely a Y_coord and it is the only “error” in this column. The code below shows the steps I took to identify and remove this outlier.\nwm.df %\u0026gt;% arrange(desc(X_coord)) #error is X_coord = 4845681.6, not a Y_coord either sort(wm.df$X_coord,decreasing = T)[1:3] # look at the top 3 largest values summary(wm.df$X_coord) #identify the error/outlier wm.df[which(wm.df$X_coord == max(wm.df$X_coord)),] #identify the row with the error/outlier wm.df \u0026lt;- wm.df[-which(wm.df$X_coord == max(wm.df$X_coord)),] #remove error 2000-01-22 Then for the Y_coord I do the same steps and discover there are three errors within this column. Again, the code below shows the steps I took to remove them.\nwm.df %\u0026gt;% arrange(Y_coord) #first three Y_coord are duplicates of X_coord sort(wm.df$Y_coord,decreasing = F)[1:3] # Y_coord errors, three wm.df[which(wm.df$Y_coord %in% sort(wm.df$Y_coord,decreasing = F)[1:3]),] wm.df \u0026lt;- wm.df[-which(wm.df$Y_coord %in% sort(wm.df$Y_coord,decreasing = F)[1:3]),] #remove these errors Now I can aggregate the data by each week, month, and date simply using the count() function from the dplyr package.\nmonth.wm \u0026lt;- wm.df %\u0026gt;% count(month) #month week.wm \u0026lt;- wm.df %\u0026gt;% count(week) #week year.wm \u0026lt;- wm.df %\u0026gt;% count(Year) #year  Data Visualization Then we can use ggplot2 create a simple plot that visualizes the number of watermain breaks per week from 1990 to 2016.\nggplot(data = week.wm, aes(x=week, y=n)) + geom_line() + labs(title = \u0026quot;Watermain Breaks in Toronto (1990-2016)\u0026quot;, x = \u0026quot;Year\u0026quot;, y = \u0026quot;Number of Breaks per Week\u0026quot;) + scale_x_datetime(date_breaks = \u0026quot;2 years\u0026quot;, date_labels = \u0026quot;%Y\u0026quot;) + theme_minimal()  It looks like there is some pattern over time with peaks occurring in a periodic fashion. Let’s take a look at the seasonality of watermain breaks by month and week. We can add a new column to our data frame that codes for the month using the mutate() function.\nmth.wm \u0026lt;- wm.df %\u0026gt;% group_by(month, Year) %\u0026gt;% count(month) %\u0026gt;% mutate(month_n = as.factor(month(month, label = T))) mthwk.wm \u0026lt;- wm.df %\u0026gt;% group_by(week, month, Year) %\u0026gt;% count(week, month, Year) %\u0026gt;% mutate(month_n = as.factor(month(month, label = T))) %\u0026gt;% mutate(yweek = week(week)) Then we can visualize this using boxplots representing the number of watermain breaks per month.\nggplot(mth.wm, aes(x = month_n, y = n)) + geom_boxplot(aes(group = month_n)) + labs(title = \u0026quot;Seasonality of Watermain Breaks in Toronto (1990-2016)\u0026quot;, x = \u0026quot;Month\u0026quot;, y = \u0026quot;Number of Breaks\u0026quot;) + theme_minimal()  Looking at the dark black line in each boxplot, we can see that there is a clear increase in the median number of watermain breaks in Toronto during the colder months (November, December, January, February). Let’s use the nifty animation package and create a .gif file to see how trend changes over the 27 year period. The boxplots in the animation below are for the entire time period from 1990 to 2016 and the changing line represents the median number of watermain breaks per month in the specified year in the title.\nlibrary(animation) aspect.w \u0026lt;- 800 aspect.r \u0026lt;- 1.6 title.size \u0026lt;- 20 ani.options(ani.width = aspect.w, ani.height = aspect.w / aspect.r, units = \u0026quot;px\u0026quot;) saveGIF({ for (i in unique(mth.wm$Year)) { g.loop \u0026lt;- ggplot(data = mth.wm, aes(x = month_n, y = n)) + geom_boxplot(aes(group = month_n)) + stat_summary( data = subset(mth.wm, Year == i), fun.y = median, geom = \u0026quot;line\u0026quot;, aes(group = 1), color = \u0026quot;#222FC8\u0026quot;, alpha = 0.6, size = 2 ) + labs( title = paste0(\u0026quot;Seasonality of Watermain Breaks in Toronto (\u0026quot;, i, \u0026quot;)\u0026quot;), x = \u0026quot;Month\u0026quot;, y = \u0026quot;Number of Breaks per Month\u0026quot; ) + theme(plot.title = element_text(size = title.size, face = \u0026quot;bold\u0026quot;)) print(g.loop) } }, movie.name = \u0026quot;wm_wm.gif\u0026quot;, interval = 0.9, nmax = 30, 2)   Basic Time Series Decomposition Since I aggregated the counts of watermain breaks by month, we have a set of data points in discrete intervals over time. In a basic sense, this is a time series and many other types of data exist in this format (stock market data, weather data, digital signal data, etc.) and there is a wealth of science and methods that deal with this type of data. That deserves a post in itself to talk about. Back to watermains… to get a better idea of the periodic pattern we observed in the first line plot and if there is a general trend over the 27 year period, we can use some R packages to “decompose” the data, extract the seasonal and trend components of the watermain break time series, and visualize them. By looking at these output plots we can make some preliminary inferences on the watermain breaks happening from 1990 to 2016.\nSince I am a ggplot2 junkie, we’ll make sure we have the ggfortify package loaded in our R session and make use of the convenient autoplot() function. First we’re going to create a time series object that contains the counts of watermain breaks per month from 1990 to 2016. The first argument in the ts() function is the vector containing the counts of watermain breaks per month. We don’t necessarily have to worry about the column containing the date information since the second argument you can specify the start date c(1990,1) or January 1990 and the third argument you can specify the intervals/frequency of each data point; in this case since our time unit is a month we set the frequency to 12.\nm.ts \u0026lt;- ts(month.wm$n, start=c(1990,1), frequency=12) Now that we have our ts object, we can use the convenient autoplot() function to do a lazy plot the data. This is the monthly counts version of the first plot we made earlier in the post (which was counts per week).\nautoplot(m.ts, main = \u0026quot;Time Series Watermain Breaks per Month in Toronto\u0026quot;) Then we use the decompose() function to extract the seasonal and trend information from the time series. The output is a list of data frames containing the original time series, the seasonal component, trend component, random component, and some other attribute information. Here I use the option to assume an additive time series model since the seasonal variation looks pretty consistent. This link has a nice summary of the steps in decomposing a basic time series if you want to read further on this. The equation that this additive model is based on is below:\n Y[t] = T[t] + S[t] + e[t]\n Where Y is the data point per time unit (month) is equal to the sum of the three components: T – trend, S – seasonality, and e – a random component. In short, the random component is the the remainder of the trend and seasonal components and can be suggestive of outliers in the data while considering the trend and seasonality.\nautoplot(decompose(m.ts, type = \u0026quot;additive\u0026quot;))  From this plot we see the original time series at the top with the facet label ‘data’ (we seen this twice now so it’s nothing new). The second plot show the seasonal component and since there is a definite repeating pattern with the peaks coinciding with the peaks of watermain breaks and we can conclude there is seasonality or periodicity with the data (and confirms what we saw with the monthly box plots). The third plot displays the trend while disregarding the seasonal component and we can see that over the time period there is a general decreasing trend of watermain breaks in Toronto. The final and fourth plot shows the remainder or random component and it describes any variation or deviation in the data that is not considered in the seasonal or trend data. We can see in the original data the high spike in 1994 has a spike in the remainder plot as well. We can make a general conclusion that there this is an extraordinary number of watermain breaks in that small window. A more systematic approach would be to predetermine an threshold in which a remainder value would be significant, since we see some deviations occurring the recent years.\n Spatial Data The dataset contains spatial information contained in the columns labelled X_coord and Y_coord. It seems like the City of Toronto uses the Modified Transverse Mercator (MTM), North American Datum 1927 (NAD27) with a truncated northing (-4,000,000) geographic coordinate system. I’m not very familiar with doing coordinate system conversions in R but I would appreciate any feedback on how to do this! Regardless of this, I decided to visualize the spatial information and get a general idea of where these breaks were happening and when. The code below takes the coordinate data and plots them using ggplot2.\nggplot(data = wm.df, aes(x = X_coord, y = Y_coord, color = year)) + geom_point(size = 1.5, alpha = 0.4) + scale_color_viridis(option = \u0026quot;B\u0026quot;) + labs( title = paste0(\u0026quot;Map of Watermain Breaks\u0026quot;), subtitle = paste0(\u0026quot;City of Toronto (1990-2016)\u0026quot;) ) + theme( axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank(), panel.background = element_rect(fill = \u0026quot;grey95\u0026quot;, colour = \u0026quot;grey5\u0026quot;), legend.background = element_rect(fill = \u0026quot;grey95\u0026quot;), panel.grid.minor = element_blank(), panel.grid.major = element_blank(), plot.title = element_text(size = title.size, face = \u0026quot;bold\u0026quot;), plot.subtitle = element_text(size = title.size * 0.6, face = \u0026quot;plain\u0026quot;), plot.background = element_rect(fill = \u0026quot;grey95\u0026quot;) )  I am also a viridis package junkie if you haven’t noticed from my other posts. So as the legend says, the brighter yellow indicates a more recent watermain break, darker ones are further back in time. This plot isn’t too information since it shows all the watermain breaks over the entire 27 year period and we don’t have any reference for exact locations within the city of Toronto.\n So what does this mean… At first glance, you might be concerned after doing some quick mental math (35,461 breaks over 27 years… ~1300 breaks per year) and conclude that tap water provided these pipes is not safe and the residents of Toronto should only drink bottled water for the rest of their lives. This is NOT the point of this post and nor does it suggest people should be overly paranoid with seeing this. The city of Toronto is well-resourced and have the capacity to response to these watermain breaks, isolate them, fix them in a timely manner, and resume service of safe drinking water. The health risk from these breaks shouldn’t concern the average Torontonian and since waterborne diseases are extremely rare in developed countries and Toronto makes sure that their four water treatment plants are fully functioning to make sure all the water is safe. If you want to more about watermain breaks in Toronto you can check out their website and read on how they are working hard behind the scene to ensure that you (if you live in Toronto) have access to safe drinking water.\nEverything in the post are my own views and as always feedback, comments, suggestions are always welcome. I am constantly learning new things about using R and love to learn. Feel free to reach out to me!\n ","date":1512777600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586625311,"objectID":"49c85866a33e0b167b939ee9af858ae7","permalink":"/post/wm-breaks/","publishdate":"2017-12-09T00:00:00Z","relpermalink":"/post/wm-breaks/","section":"post","summary":"Original post date: December 09 2017\nThis post goes into using open data provided by the City of Toronto and all the code is found on Github.\nDrinking Water Water is a basic physiological need for humans. We can go days without food and be okay, but without water… that’s another story. Drinking water or potable water is something in the developed world we take for granted. Much of the world does not have great access to clean water and by United Nations had made their 6th Sustainable Development Goal to “ensure access to water and sanitation for all” and that all people would have universal access to safe and affordable water by 2030.","tags":[],"title":"Watermain Breaks in the City of Toronto","type":"post"},{"authors":null,"categories":["R","Public Health"],"content":" Original post date: May 25 2017\nI had the opportunity to attend the Open Data Science Conference (ODSC) East held in Boston, MA. Over a two day period I had the opportunity to listen to a number of leaders in various industries and fields. It was inspiring to learn about the wide variety of data science applications ranging from finance and marketing to genomics and even the refugee crisis.\nOne of the workshops at ODSC was text analytics, which includes basic text processing, dendrograms, natural language processing and sentiment analysis. This gave me the thought of applying some text analytics to visualize some data I was working on last summer. In this post I’m going to walk through how I used regular expression to label classification codes in a large dataset (NHAMCS) representing emergency department visits in the United States and eventually visualize the data.\nThe NHAMCS and ICD-9 Codes Without going into too much detail, the National Health Ambulatory Medical Care Survey (NHAMCS) is large sample survey used to provide an estimate of visits to outpatient and emergency departments in general/short-stay hospital at a national scale for the United States. Those who are well versed in complex probability-sample survey designs (or interested in it), you can read about how it’s set up here. For this post I only using the emergency department data, not outpatient. The main thing to keep in mind moving forward is that the survey’s basic unit of measurement are visits or encounters made in the United States to emergency departments.\nDiagnostic information on each visit is collected and coded in the NHAMCS by using the International Classification of Diseases (ICD) codes maintained by the World Health Organization. Specifically the NHAMCS public data files (1992-2014) use the ICD, 9th revision, clinical modification (ICD-9-CM). The 10th revision being currently used throughout the world since 2015 and the 11th revision is in-process, most likely it will be disseminated in 2018. You can check out this package I recently came across that validates and searches co-morbidities of ICD-9 and ICD-10 codes in R.\n The Data Game Plan In this post, the code is only for the 2005 emergency department (ED) NHAMCS dataset. If you interested in getting the original data you can go to this page. There is extensive documentation on the NHAMCS website that helps you download the data for use in SAS, Stata and SPSS. From my current knowledge (hours searching the vast interweb) there is no method in R to download these files. If someone wants to take the challenge of developing a method of pulling down the data using R, go for it! For this post, the data I’m using were Stata files and I used an R package to read-in those files. I’ve included subsetted data for ED NHAMCS from 2005 to 2009 in my GitHub repository if anyone is interested in using those files with my code.\nTo achieve our goal of visualizing diagnoses in ED visits we need to obtain, clean, and then appropriately setup our data. First we’re going to parse the text in official ICD-9-CM documentation using regular expression to create our own ICD dictionary/lookup table. Next, we’ll take the 2005 ED NHAMCS data, apply the survey design to get the correct estimates per ED visit. Then we’ll use our ICD dictionary to define each code, set it all up using the data.table package to create a Treemap visualization. I’ll be explaining a fair amount on data cleaning and regular expression, if you want to see the treemap you can scroll down to the end of the post.\n Using Regex on ICD-9 Codes Regular expression or regex is not unique just to R but is used across other types of programming languages (Java, Python, Perl). Simply put, regex is used to extract patterns of characters in a string. There is a good tutorial video here on regex and another video here on using regex in R. Disclaimer: I am not an expert on regex! I am still learning and this post is based on my current knowledge I’ve gathered so far. A good and helpful source I’ve been using is Rexegg. Regardless, let’s march forward towards our goal!\nI’m going to take the 3-digit grouping ICD-9-CM documentation from the CDC’s Health Statistics [website]ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Publications/ICD9-CM/2010), where I downloaded the DINDEX11.zip file. Because of the annoying nature of the rich-text files (RTF), I just copied the entire document into a basic UTF-8 text file called Dc_3d10.txt. My entire code is in my GitHub, but I’ll be referring to particular sections in this post. So moving forward, I’ll read this into R using the readLines function and this output…\nicd9.3 \u0026lt;- readLines(\u0026quot;Dc_3d10.txt\u0026quot;) \u0026gt; head(icd9.3,15) # [1] \u0026quot;Appendix E\\u2028List of Three-Digit Categories\u0026quot; # [2] \u0026quot;1.\\tINFECTIOUS AND PARASITIC DISEASES\u0026quot; # [3] \u0026quot;Intestinal infectious diseases (001-009)\u0026quot; # [4] \u0026quot;001\\tCholera\u0026quot; # [5] \u0026quot;002\\tTyphoid and paratyphoid fevers\u0026quot; # [6] \u0026quot;003\\tOther salmonella infections\u0026quot; # [7] \u0026quot;004\\tShigellosis\u0026quot; # [8] \u0026quot;005\\tOther food poisoning (bacterial)\u0026quot; # [9] \u0026quot;006\\tAmebiasis\u0026quot; # [10] \u0026quot;007\\tOther protozoal intestinal diseases\u0026quot; # [11] \u0026quot;008\\tIntestinal infections due to other organisms\u0026quot; # [12] \u0026quot;009\\tIll-defined intestinal infections\u0026quot; # [13] \u0026quot;Tuberculosis (010-018)\u0026quot; # [14] \u0026quot;010\\tPrimary tuberculous infection\u0026quot; # [15] \u0026quot;011\\tPulmonary tuberculosis\u0026quot; It looks like there is a header/title at [1], numeric grouping at [2] “1.AND PARASITIC DISEASES”, subgrouping by ICD-9 code ranges, at [3] “Intestinal infectious diseases (001-009)” and then 3-digit ICD-9 codes followed by a specific diagnosis, at [10] “007protozoal intestinal diseases”. At the end we want to produce three separate data frames that we’ll categorize as:\nGroups: the title which contains the general diagnosis grouping\n Subgroups: the range of ICD-9 codes that contain a certain diagnosis subgroup\n Classification: the specific 3-digit ICD-9 code that corresponds with a diagnosis  First we’ll use a simple operation to remove the first line:\nicd9.3 \u0026lt;- icd9.3[-1] Next we’ll use regex to extract the Groups using this…\nicd9.3[grep(\u0026quot;^[0-9]+\\\\.\u0026quot;,icd9.3)] The grep() function is used to extract patterns based on regex. To explain the regex, I am selecting lines that start with a digit of any length which is immediately followed by a period. The carat ^ initiates the pattern matching from the start of the string (going left to right). The [0-9] represents any digit from 0 through 9 and the plus sign + denotes at least one repeat of any digit. The two backslashes \\\\ is required because periods in regex are special meta-characters that represent any character (number, letter, symbol). To summarize, this identifies strings that start with 1., 2., 3., …, 17., and beyond.\nNext I use the gsub() function, which replaces or substitutes based on regex to replace all the \\t in the group names and replace it with two underscore __ which we’ll use later as an identifier to split the string in half to make the data frame (if this isn’t clear right now, hopefully it will be when we do the splitting).\nicd9.3g \u0026lt;- gsub(\u0026quot;\\\\.\\t\u0026quot;,\u0026quot;__\u0026quot;,icd9.3g) \u0026gt; icd9.3g[1:3] # [1] \u0026quot;1__INFECTIOUS AND PARASITIC DISEASES\u0026quot; # [2] \u0026quot;2__NEOPLASMS\u0026quot; # [3] \u0026quot;3__ENDOCRINE, NUTRITIONAL AND METABOLIC DISEASES, AND IMMUNITY DISORDERS\u0026quot; This regex removes all \\t specifically when it follows a period. Here we use the \\\\ again to identify a period.\nNow we’ll split the string into separate columns within a data frame using the str_split_fixed function in the stringr package. I set stringsAsFactors = FALSE so that we maintain the data type of character with the strings. You can read about how this code has caused headaches for many users here. Here I assign Group.n for the number\ndiag.g \u0026lt;- as.data.frame(stringr::str_split_fixed(icd9.3g,\u0026quot;__\u0026quot;,2),stringsAsFactors = F) names(diag.g) \u0026lt;- c(\u0026quot;Group.n\u0026quot;,\u0026quot;Group\u0026quot;) diag.g \u0026lt;- diag.g[c(\u0026quot;Group\u0026quot;,\u0026quot;Group.n\u0026quot;)] #swap order of columns \u0026gt; head(diag.g) # Group Group.n # 1 INFECTIOUS AND PARASITIC DISEASES 1 # 2 NEOPLASMS 2 # 3 ENDOCRINE, NUTRITIONAL AND METABOLIC DISEASES, AND IMMUNITY DISORDERS 3 # 4 DISEASES OF BLOOD AND BLOOD-FORMING ORGANS 4 # 5 MENTAL DISORDERS 5 # 6 DISEASES OF THE NERVOUS SYSTEM AND SENSE ORGANS 6 Now we have a two-column data frame that contains the group number and name of that group so we can refer to for the future. However, later on (I realized when I got there) I needed to create additional columns to account for the range of 3-digit values that correspond to each Group. To do this I used the grep() function to first get the names of each Group and the index of the Group titles in the text file by changing the value argument in grep().\ngroupn \u0026lt;- grep(\u0026quot;\\\\.\\t\u0026quot;,icd9.3,value=T) #names groupi \u0026lt;- grep(\u0026quot;\\\\.\\t\u0026quot;,icd9.3,value=F) #index Afterwards I find the lower and upper limit of each Group range by using the index and simple vector arithmetic in combination of gsub() to extract only numbers. For the upper limit and added the maximum value of 999 (only 3-digits remember!) so that the lengths match, since I didn’t extract it from the code above. Then create the final dictionary for the Groups.\nlow.g \u0026lt;- gsub(\u0026quot;[^0-9]\u0026quot;,\u0026quot;\u0026quot;,icd9.3[groupi+2]) #lower limit of range up.g \u0026lt;- c(gsub(\u0026quot;[^0-9]\u0026quot;,\u0026quot;\u0026quot;,icd9.3[groupi-1]),\u0026quot;999\u0026quot;) #upper limit of range diag.g \u0026lt;-data.frame(diag.g,low.g,up.g, stringsAsFactors = F) names(diag.g) \u0026lt;- c(\u0026quot;Group\u0026quot;,\u0026quot;Group.n\u0026quot;,\u0026quot;Start\u0026quot;,\u0026quot;End\u0026quot;) We move on to the Subgroups which we will extract based on the fact that all the corresponding lines have a parentheses contain a pattern of… 3 digits, a dash then another 3 digits. Ie. “Fracture of skull (800-804)” The appropriate regex is as follows…\nicd9.3sg \u0026lt;- icd9.3[grep(\u0026quot;\\\\([0-9]{3}-[0-9]{3}\\\\)\u0026quot;,icd9.3)] The double backslashes are used for parentheses (just like for periods) and I specify the pattern of any 3 digits using the {} curly brackets. Next we remove any lingering \\t that might exist and replace with a space \u0026quot; \u0026quot;. After that we’ll insert the __ split identifier we used above. This is specified by using regex to match the pattern of 3-digit parentheses that follows directly after a space, which is denoted as \\\\s.\nicd9.3sg \u0026lt;- gsub(\u0026quot;\\t\u0026quot;,\u0026quot; \u0026quot;,icd9.3sg) icd9.3sg \u0026lt;- gsub(\u0026quot;\\\\s\\\\(([0-9]{3}-[0-9]{3})\\\\)\u0026quot;,\u0026quot;__\\\\1\u0026quot;,icd9.3sg) Like we did for the Group above, we’ll split the based on the __ split identifier.\ndiag.sg \u0026lt;- as.data.frame( stringr::str_split_fixed(icd9.3sg,\u0026quot;__\u0026quot;,2), stringsAsFactors = F) After that, we even want to split the ranges by the dash - so we can get the ‘start’ and ‘end’ of the ICD-9 code range for each Subgroup. Then we’ll assign the names of the columns in the newly created data frame.\ndiag.sg \u0026lt;- as.data.frame( cbind( diag.sg, stringr::str_split_fixed(diag.sg[,2],\u0026quot;-\u0026quot;,2), stringsAsFactors = F)) names(diag.sg) \u0026lt;- c(\u0026quot;Subgroup\u0026quot;,\u0026quot;Range\u0026quot;,\u0026quot;Start\u0026quot;,\u0026quot;End\u0026quot;) \u0026gt; head(diag.sg,4) # Subgroup \u0026amp;nbsp; Range Start End # 1 Intestinal infectious diseases 001-009 001 009 # 2 Tuberculosis 010-018 010 018 # 3 Zoonotic bacterial diseases 020-027 020 027 # 4 Other bacterial diseases 030-041 030 041 Now we’ll extract the Classifications. We want to generate a regex that captures a pattern of:\nA 3-digit sequence followed by a \\t  We can use the “|” or the OR operator in regex to capture these 3 patterns. Using similar regex from the Groups and Subgroups we generate this…\nicd9.3c \u0026lt;- icd9.3[grep(\u0026quot;(^[0-9]{3}\\t)|(^V[0-9]{2}\\t)|(E[0-9]{3}\\t)\u0026quot;,icd9.3)] And then we can do the same method as the Group to remove any lingering “” and split the string to get an output of a data frame. Also\nicd9.3c \u0026lt;- gsub(\u0026quot;\\t\u0026quot;,\u0026quot;__\u0026quot;,icd9.3c) diag.c \u0026lt;- as.data.frame(stringr::str_split_fixed(icd9.3c,\u0026quot;__\u0026quot;,2),stringsAsFactors = F) names(diag.c) \u0026lt;- c(\u0026quot;Code\u0026quot;,\u0026quot;Classification\u0026quot;) diag.c \u0026lt;- diag.c[c(\u0026quot;Classification\u0026quot;,\u0026quot;Code\u0026quot;)] \u0026gt; head(diag.c) # Classification Code # 1 Cholera 001 # 2 Typhoid and paratyphoid fevers 002 # 3 Other salmonella infections 003 # 4 Shigellosis 004 # 5 Other food poisoning (bacterial) 005 # 6 Amebiasis 006 We started with some messy text but now after all that regex we have successfully extracted 3 separate data frames for the ICD-9 Group, Subgroup and Classification that can used as a reference moving forward!\n Applying Survey Design Since this post is focusing on regex, I will not go into detail about applying survey design to NHAMCS data files using R. In short, I used the survey package to apply the appropriate weighting based on the 4-stage probability sample survey implemented by the NHAMCS. I’ve annotated my code for those who are interested about it but for the sake of this post I will not talk about it any further. All the details are based in the documentation files, that you can find by year in this link.\nWhy do we have to apply the weighting? By doing this, we can actually obtain accurate national estimates of ED visits in the United States. If we started running descriptive statistics on our data without this step, we’ll only be analyzing unweighted counts, which might not be an accurate representation of the sampling population. For those interested can read this post on about weighted surveys in R here.\n Matching Codes to Descriptions For the sake of this analysis we’ll only look at the first column of ICD-9 codes in the 2005 ED NHAMCS dataset, denoted as DIAG1.\n# \u0026gt; ed05$DIAG1[1:8] # [1] \u0026quot;8472-\u0026quot; \u0026quot;78099\u0026quot; \u0026quot;V997-\u0026quot; \u0026quot;V997-\u0026quot; \u0026quot;7931-\u0026quot; \u0026quot;8489-\u0026quot; \u0026quot;920--\u0026quot; \u0026quot;8920-\u0026quot; At first glance we notice two main things…\nFirst, that each set of strings are five characters long. This is because the ICD-9 code system allows for more specificity with diagnoses beyond 3-digits. Take a quick look at the ICD-9 Wiki page for hypertensive disease. You can see that secondary hypertension can be further classified to malignant vs. benign and again whether it is renovascular or not. These classifications use up 5 digits (if you ignore the periods).\nSecondly, that there are dashes when there are less than five characters. If we were only working with uniform patterns the match function would have been a simple solution (link to a short explanatory post), but this is not the case. It looks like we’ll have to use regex again to look at the first 3 characters in each element of the DIAG1 vector.\nI created a loop to match the 3-digit Classification code to the DIAG1 column. I created a new column and filled it with the appropriate code using regex matching. The last two lines are included to help us finalize our dictionary using data.table package.\ned05.df$Code \u0026lt;- \u0026quot;-\u0026quot; for (k in paste0(\u0026quot;^\u0026quot;,diag.c[,2])){ #codes gindex \u0026lt;- grep(k,ed05.df[,1]) #matches code with DIAG1 column ed05.df$Code[gindex] \u0026lt;- diag.c$Code[grep(k,diag.c[,2])] ed05.df$Code.n \u0026lt;- as.numeric(ed05.df$Code) #some values are coered to NA ed05.df$Code.n[which(is.na(ed05.df$Code.n))] \u0026lt;- 1000 #treat all NA\u0026#39;s wih value 1000 } Then we use the join() function from the plyr package to match the Classifications with the codes in our new data frame.\ned05.df \u0026lt;- join(ed05.df,diag.c,by=\u0026quot;Code\u0026quot;)  Overlap Joins using data.table I’m not going to get into much detail here because this was something I had to learn when I discovered that joins and merges based on character types (ie. “001” or “057”) were tedious and very annoying. Thankfully this StackOverflow post was extremely useful to understand how the foverlaps() function works. In short to address this issue, I created numeric equivalents of the ICD-9 codes (ie. “019” became 19) because foverlaps() only works with numeric and integer values. After that I assigned the appropriate categories of Group and Subgroup to the Classification codes, cleaned it up a bit to remove unnecessary columns, and convert it back into a data.frame class. My code is annotated if you would want to look at it and I would refer to the post I mentioned above.\n Visualizing the Data Since we have somewhat hierarchical data we can visualize this using a treemap. Now we make sure the treemap package is loaded and then use the lovely IWantHue website to get a nice color palette. I’ve annotated the code to explain what each argument does and after running this we get…\ntree.n \u0026lt;- treemap(df.ed05, index = c(\u0026quot;Group\u0026quot;, \u0026quot;Subgroup\u0026quot;, \u0026quot;Classification\u0026quot;), #grouping for each vSize = \u0026quot;Freq\u0026quot;, #area of rectangles by NHAMCS estimates type = \u0026quot;index\u0026quot;, #see documentation title = \u0026quot;NHAMCS Emergency Department Visits in 2005\u0026quot;, overlap.labels = 0.5, ##see documentation palette = pal1, #custom palette from IWantHue border.col = c(\u0026quot;#101010\u0026quot;, \u0026quot;#292929\u0026quot;, \u0026quot;#333333\u0026quot;), #set border colors fontsize.title = 40, #set font size fontsize.labels = c(30, 24, 16), #set size for each grouping lowerbound.cex.labels = .4, ##see documentation fontcolor.labels = c(\u0026quot;#000000\u0026quot;, \u0026quot;#292929\u0026quot;, \u0026quot;#333333\u0026quot;), #set color fontface.labels = c(2, 4, 1), # bold, bold-italic, normal fontfamily.labels = c(\u0026quot;sans\u0026quot;), #sans font inflate.labels = F, #see documentation align.labels = c(\u0026quot;center\u0026quot;, \u0026quot;center\u0026quot;), #align all labels in center bg.labels = 230 # 0 and 255 that determines the transparency )  From this we see that most ED visits had a diagnosis related to injury, poisoning, symptoms and respiratory system issues. This makes sense because usually you go to the hospital when you have an injury or some acute medical emergency. Below I’ve also created the treemaps for the other years leading up to 2009 with the colors corresponding with each group.\nSome comments on the data and visualization:\n It does not include ICD-9 codes for Supplementary Classifications (codes that start with V’s and E’s). I ignored these due to tedious nature of matching the Groups with characters and maybe in the future I can write something that includes them to the visualization.\n This data is for all visits regardless of demographic information. I suspect the treemaps between age, gender, race, and region would look very different.\n These are national estimates and the true value is contained within a confidence interval dependent on the survey design. Other visualizations may be appropriate to show the probability ranges of this data.\n There were missing or blank ICD-9 codes (the coders at the hospitals either didn’t fill them or there were no diagnoses?) accounted for 2.78% of all estimates.\n Originally I wanted to create a radial or sunburst tree of the data, but I had difficulty setting up the data in an appropriate form (ie. classes Node to phylo). I would appreciate any knowledge on how to get around this and maybe make that my next mini-project.\n This data is from ~10 years ago, it would be interesting to see what the most recent NHAMCS data says about emergency department visits.   Final Thoughts From ODSC a speaker said that for data visualization, 80% of your work is cleaning setting up the data for the visualization and 20% is actually making the visualization… very true for this case. I did not expect the regex and data.table to take as long as it did. Regardless, I got the chance to learn more about the data.table package, practice my regular expression knowledge, and explore different tree visualizations! Any feedback, comments, and questions on the code or visualization are always welcome!\n  ","date":1495670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586625311,"objectID":"ac1355c1af7a19c31c722b3072d78c39","permalink":"/post/ed-visits-regex-treemaps/","publishdate":"2017-05-25T00:00:00Z","relpermalink":"/post/ed-visits-regex-treemaps/","section":"post","summary":"Original post date: May 25 2017\nI had the opportunity to attend the Open Data Science Conference (ODSC) East held in Boston, MA. Over a two day period I had the opportunity to listen to a number of leaders in various industries and fields. It was inspiring to learn about the wide variety of data science applications ranging from finance and marketing to genomics and even the refugee crisis.","tags":[],"title":"Regular Expression \u0026 Treemaps to Visualize Emergency Department Visits","type":"post"},{"authors":[],"categories":["R","Public Health"],"content":" Original post date: March 03 2017\nFor this post, as similar to previous ones, I give a guide through the process I took to organize the data, a nifty function I created to search the documentation and a visualization of the data using ggplot2.\nWhat is Tuberculosis? Tuberculosis (TB) is a nasty disease caused by the bacterium Mycobacterium tuberculosis. It was first described by Hippocrates and has a history concurrent with human civilization. Individuals infected with TB usually do not develop active disease but if they develop active TB, they can on average infect 8-10 people. Most of the time when we hear of TB, we think of the infection within the lungs, but I learned during my time volunteering in a medical clinic in Peru that TB can manifest in other sites of the body (extra-pulmonary TB). This bacterium has infected roughly a third of the world’s population and is a top 10 cause of death around the globe.\nThere are two major modern concerns with TB…\nTB and HIV co-infection, where there is a disproportionate mortality rate by TB with HIV infected individuals. HIV infects CD4+ T-cells, which are essential for an appropriate immune response to combat TB. This relationship has been reported in patients.\n Multi-drug resistant TB (MDR-TB) is a growing public health concern that threatens the control of TB infection throughout the world. There have been reports of extensive-drug resistant TB (XDR-TB) which is resistant to the standard treatment of isoniazid and rifampin alongside any fluoroquinolone and at least one secondary-treatment drug. Fewer treatment options put patients under rigorous and strenuous regimens and present a risk of transmission to others.   Data Retrieval The data I used for this post is taken from the World Health Organization from this link to the TB data. Specifically I used the data dictionary and TB burden estimates. When you click these links, they generate a comma-delimited file (.csv) and date-stamp the file. I downloaded these files back in February 2016. Just remember where you save these in your local directory so you can read them into R for the near future.\n Getting Started The code I use for my posts can be found on my GitHub account, open for free use and adaptation. I’m always open to comments and suggestions on how to improve my code or even a completely different method to reach the same goal. Optimization of R programming is something I am working towards bit by bit.\nAfter setting up your working directory, read your comma-delimited files using the read.csv() function. The WHO files are conveniently formatted so there is no need for elaborate arguments for headers, field separators or assigning missing values.\nTB.burden \u0026lt;- read.csv(\u0026quot;TB_burden_countries_2016-02-18.csv\u0026quot;) #TB burden dataset TB.dic \u0026lt;- read.csv(\u0026quot;TB_data_dictionary_2016-02-18.csv\u0026quot;) #TB documentation dictionary  Customized Search Function While completing an R assignment for one of my classes, I ran into the readline() function which allows for some interactive functionality within R. The reason I created this interactive custom search function is because of the fact that the column variables in the TB burden .csv file are not completely straightforward and it’s annoying to manually search through the file itself in Microsoft Excel. So instead of memorizing all of them (only to forget after I write 10 lines of code), I decided to create this function whenever I start to write code for a new plot and need to remind myself the exact description of the variable I am working with…\nTB.search \u0026lt;- function (x){ # look \u0026lt;- names(x) #assign the names/column names of the subsetted data message(\u0026quot;Matching variable names found in the documentation files\u0026quot;) TB.d \u0026lt;- as.character(TB.dic[,4]) # conversion to character to have \u0026quot;clean\u0026quot; output out \u0026lt;- look[look %in% TB.dic[,1]] #assignment of vector: which names of the selected subset are in the documentation file print(out) #check which variable names from dictionary match the input message(\u0026quot;Above includes all names for variable names found in the documentation files\u0026quot;) #message prompt for output z \u0026lt;- readline(\u0026quot;Search Variable Definition (CASE-SENSITIVE): \u0026quot;) #interactive read-in of variable name if (z %in% out){ # NEED TO MAKE THE CONDITION TO SEPARATE integer(0) from real integer values print(TB.d[grep(paste0(\u0026quot;^(\u0026quot;,z,\u0026quot;){1}?\u0026quot;),TB.dic[,1])]) #print the definition found in TB.d }else{ repeat{ #repeat the line below for readline() if exact match of doesn\u0026#39;t exist y\u0026lt;-readline(\u0026quot;Please re-input (press \u0026#39;ESC\u0026#39; to exit search): \u0026quot;) #second prompt if (y %in% out) break # the condition that ends the repeat, that the input matches a name in the documentation } print(TB.d[grep(paste0(\u0026quot;^\u0026quot;,y,\u0026quot;{1}?\u0026quot;),TB.dic[,1])]) #print the definition found in documentation file TB.dic } } I’m only going to highlight a couple things in the code above. I converted the definitions found in the documentation file because the original data type was a factor. This was a problem with the print output because it would show all the unnecessary information on the number of factors and values of the factors I had no interest in. Secondly I ended up using a repeat in my if statement to prompt the user if they incorrectly typed in a variable name in the console. This link provides some helpful information on the repeat loop. Other then that, I used regular expression to specify the search to match what was actually in the documentation file.\n Cleaning Data As with all data, it doesn’t come in the form that you always want it in. Luckily the WHO TB data wasn’t in bad shape and I just converted all integer valued columns into numeric types using a simple loop.\n#convert all the rows with type \u0026#39;integer\u0026#39; to \u0026#39;numeric for (k in 1:length(names(TB.burden))){ if (is.integer(TB.burden[,k])){ TB.burden[,k] \u0026lt;- as.numeric(TB.burden[,k]) } } After that, I used the mapvalues() function from the plyr package to change the levels in the country variable. I used this because the basic functions in R require me to specify all the levels in the factor and using 219 country names seemed tedious. Quite simply, I just listed the old names I wanted to change and created a respective character vector of the the new country names.\nlibrary(plyr) TB.burden$country \u0026lt;- (mapvalues((TB.burden$country), from = c( \u0026quot;Bolivia (Plurinational State of)\u0026quot;, \u0026quot;Bonaire, Saint Eustatius and Saba\u0026quot;, \u0026quot;China, Hong Kong SAR\u0026quot;, \u0026quot;China, Macao SAR\u0026quot;, \u0026quot;Democratic People\u0026#39;s Republic of Korea\u0026quot;, \u0026quot;Democratic Republic of the Congo\u0026quot;, \u0026quot;Iran (Islamic Republic of)\u0026quot;, \u0026quot;Lao People\u0026#39;s Democratic Republic\u0026quot;, \u0026quot;Micronesia (Federated States of)\u0026quot;, \u0026quot;Republic of Korea\u0026quot;, \u0026quot;Saint Vincent and the Grenadines\u0026quot;, \u0026quot;Sint Maarten (Dutch part)\u0026quot;, \u0026quot;The Former Yugoslav Republic of Macedonia\u0026quot;, \u0026quot;United Kingdom of Great Britain and Northern Ireland\u0026quot;, \u0026quot;United Republic of Tanzania\u0026quot;, \u0026quot;Venezuela (Bolivarian Republic of)\u0026quot;, \u0026quot;West Bank and Gaza Strip\u0026quot;) , to = c( \u0026quot;Bolivia\u0026quot;, \u0026quot;Caribbean Netherlands\u0026quot;, \u0026quot;Hong Kong\u0026quot;, \u0026quot;Macao\u0026quot;, \u0026quot;North Korea\u0026quot;, \u0026quot;DRC\u0026quot;, \u0026quot;Iran\u0026quot;, \u0026quot;Laos\u0026quot;, \u0026quot;Micronesia\u0026quot;, \u0026quot;South Korea\u0026quot;, \u0026quot;St. Vincent \u0026amp;amp;amp;amp;amp; Grenadines\u0026quot;, \u0026quot;Sint Maarten\u0026quot;, \u0026quot;Former Yugoslav (Macedonia)\u0026quot;, \u0026quot;UK \u0026amp;amp;amp;amp;amp; Northern Ireland\u0026quot;, \u0026quot;Tanzania\u0026quot;, \u0026quot;Venezuela\u0026quot;, \u0026quot;West Bank and Gaza\u0026quot;) )) I threw in some random subsetting into the code to pull out values for specific questions (ie. what was the estimated prevalence per 100,000 people of TB in Peru during 2014). You can see it in the actual code in the GitHub repository.\n Visualizing the Data I was interested in what estimated rates of TB were by WHO region, so I ran a simple loop to assign new variable names to each region (this is not the best practice IMO but it does the trick).\nfor (i in levels(TB.burden$g_whoregion)){ #selection of the WHO regions (AFR, AMR, EMR, EUR, SEA, WPR) nam \u0026lt;- paste0(i,\u0026quot;_whoregion\u0026quot;) #creation of the name based on acronyms from levels assign(nam, subset(TB.burden,TB.burden$g_whoregion %in% paste(i))) #assigning each level a subset based on the region } ls() #just to show what variables are stored... #look for AFR_whoregion, AMR_whoregion, EMR_whoregion, EUR_whoregion, SEA_whoregion, WPR_whoregion Now we set up the themes we want to use for ggplot2. I set the year scale for the legends, customized the text on the axes and assigned dark_t to be a dark, black-grey background and light_t as a simple, white-grey background theme.\nctext \u0026lt;- theme(axis.ticks = element_blank(), plot.title=element_text(family=\u0026quot;Verdana\u0026quot;), axis.text.x=element_text(size=9,family=\u0026quot;Verdana\u0026quot;), #http://www.cookbook-r.com/Graphs/Fonts/#table-of-fonts axis.title.x=element_text(size=10), axis.text.y=element_text(size=7.5,family=\u0026quot;Verdana\u0026quot;), axis.title.y-element_text(size=10,family=\u0026quot;Verdana\u0026quot;), legend.title=element_text(size=9,family=\u0026quot;Verdana\u0026quot;,vjust=0.5), #specify legend title color legend.text=element_text(size=7,family=\u0026quot;Verdana\u0026quot;,face=\u0026quot;italic\u0026quot;) ) dark_t \u0026lt;- theme_minimal()+ theme( plot.background = element_rect(fill=\u0026quot;#191919\u0026quot;), #plot background panel.border = element_blank(), #removes border panel.background = element_rect(fill = \u0026quot;#000000\u0026quot;,colour=\u0026quot;#000000\u0026quot;,size=2), #panel background and border panel.grid = element_line(colour = \u0026quot;#333131\u0026quot;), #panel grid colours panel.grid.major = element_line(colour = \u0026quot;#333131\u0026quot;), #panel major grid color panel.grid.minor = element_blank(), #removes minor grid plot.title=element_text(size=16, face=\u0026quot;bold\u0026quot;,family=\u0026quot;Verdana\u0026quot;,hjust=0,vjust=1,color=\u0026quot;#E0E0E0\u0026quot;), #set the plot title plot.subtitle=element_text(size=12,face=c(\u0026quot;bold\u0026quot;,\u0026quot;italic\u0026quot;),family=\u0026quot;Verdana\u0026quot;,hjust=0.01,color=\u0026quot;#E0E0E0\u0026quot;), #set subtitle axis.text.x = element_text(size=9,angle = 0, hjust = 0.5,vjust=1,margin=margin(r=10),colour=\u0026quot;#E0E0E0\u0026quot;), # axis ticks axis.title.x = element_text(size=11,angle = 0,colour=\u0026quot;#E0E0E0\u0026quot;), #axis labels from labs() below axis.text.y = element_text(size=9,margin = margin(r=5),colour=\u0026quot;#E0E0E0\u0026quot;), #y-axis labels legend.title=element_text(size=11,face=\u0026quot;bold\u0026quot;,vjust=0.5,colour=\u0026quot;#E0E0E0\u0026quot;), #specify legend title color legend.background = element_rect(fill=\u0026quot;#262626\u0026quot;,colour=\u0026quot;#383838\u0026quot;, size=.5), #legend background and cborder legend.text=element_text(colour=\u0026quot;#E0E0E0\u0026quot;)) + #legend text colour # plot.margin=unit(c(t=1,r=1.2,b=1.2,l=1),\u0026quot;cm\u0026quot;)) #custom margins ctext light_t \u0026lt;- theme(legend.background = element_rect(fill=\u0026quot;grey95\u0026quot;,colour=\u0026quot;grey70\u0026quot;, size=.5), plot.title=element_text(size=16, face=\u0026quot;bold\u0026quot;,family=\u0026quot;Verdana\u0026quot;,hjust=0,vjust=1), #set the plot title plot.subtitle=element_text(size=12,face=c(\u0026quot;bold\u0026quot;,\u0026quot;italic\u0026quot;),family=\u0026quot;Verdana\u0026quot;,hjust=0.01), #set subtitle panel.grid.minor = element_blank()) + #removes minor grid) ctext # specify scale for year variable year.scale \u0026lt;- c(1990,1995,2000,2005,2010,2014) # used for breaks below For the next plots, I’ll only be focusing on the WHO Region of Africa. Two notes on the code below for the plots: 1) I used the forcats package to reverse the order of the countries on the y-axis. This package is useful when using ggplot2 for categorical data. 2) For the legend scales, I used the comma() function in the scales package to include the comma’s for every thousand. The following code displays the estimated prevalence, mortality and incidence (per 100,000 people) for cases from each country in the WHO Africa Region.\n#PREVALENCE per 100k library(forcats) #https://blog.rstudio.org/2016/11/14/ggplot2-2-2-0/ pAFR \u0026lt;-ggplot(AFR_whoregion,aes(y=fct_rev(country),x=e_prev_100k)) + # prevalence (100k) labs(title=\u0026quot;WHO | Tuberculosis Data\u0026quot;, subtitle=\u0026quot;Africa Region\u0026quot;, x=\u0026quot;Estimated Prevalence (per 100,000)\u0026quot;,y=\u0026quot;\u0026quot;,colour=\u0026quot;#E0E0E0\u0026quot;) + geom_point(shape=20,aes(colour=year,fill=year),alpha=0.6,size=3) + scale_y_discrete(expand=c(0.002, 1))+ scale_x_continuous(limits=c(0,1500),labels=scales::comma)+ scale_fill_gradient(guide_legend(title=\u0026quot;Year\u0026quot;),breaks=year.scale,low=\u0026quot;#F9FA00\u0026quot;,high=\u0026quot;#8C00E6\u0026quot;) + #assigns colours to fill in aes scale_colour_gradient(guide_legend(title=\u0026quot;Year\u0026quot;),breaks=year.scale,low=\u0026quot;#F9FA00\u0026quot;,high=\u0026quot;#8C00E6\u0026quot;) + #assigns colours to colour in aes dark_t #MORTALITY (including HIV) per 100k e_inc_tbhiv_100k mAFR \u0026lt;- ggplot(AFR_whoregion,aes(y=fct_rev(country),x= e_mort_exc_tbhiv_100k)) + # Mortality exclude HIV (100k) labs(title=\u0026quot;WHO | Tuberculosis Data\u0026quot;, subtitle=\u0026quot;Africa Region\u0026quot;,x=\u0026quot;Estimated Mortality excluding HIV (per 100,000)\u0026quot;,y=\u0026quot;\u0026quot;,colour=\u0026quot;#E0E0E0\u0026quot;)+ geom_point(shape=20,aes(colour=year,fill=year),alpha=0.6,size=3) + scale_y_discrete(expand=c(0.002, 1)) + #scale_x_continuous(limits=c(0,250),labels=scales::comma) + scale_fill_gradient(guide_legend(title=\u0026quot;Year\u0026quot;),breaks=year.scale,low=\u0026quot;#F9FA00\u0026quot;,high=\u0026quot;#8C00E6\u0026quot;) + #assigns colours to fill in aes scale_colour_gradient(guide_legend(title=\u0026quot;Year\u0026quot;),breaks=year.scale,low=\u0026quot;#F9FA00\u0026quot;,high=\u0026quot;#8C00E6\u0026quot;) + #assigns colours to colour in aes dark_t #INCIDENCE per 100k iAFR \u0026lt;- ggplot(AFR_whoregion,aes(y=fct_rev(country),x= e_inc_100k)) + # Mortality exclude HIV (100k) labs(title=\u0026quot;WHO | Tuberculosis Data\u0026quot;, subtitle=\u0026quot;Africa Region\u0026quot;,x=\u0026quot;Estimated Incidence (per 100,000)\u0026quot;,y=\u0026quot;\u0026quot;,colour=\u0026quot;#E0E0E0\u0026quot;)+ geom_point(shape=20,aes(colour=year,fill=year),alpha=0.6,size=3) + scale_y_discrete(expand=c(0.002, 1)) + scale_x_continuous(limits=c(0,1500),labels=scales::comma) + scale_fill_gradient(guide_legend(title=\u0026quot;Year\u0026quot;),breaks=year.scale,low=\u0026quot;#F9FA00\u0026quot;,high=\u0026quot;#8C00E6\u0026quot;) + #assigns colours to fill in aes scale_colour_gradient(guide_legend(title=\u0026quot;Year\u0026quot;),breaks=year.scale,low=\u0026quot;#F9FA00\u0026quot;,high=\u0026quot;#8C00E6\u0026quot;) + #assigns colours to colour in aes dark_t Now the plots in the order of estimated prevalence, incidence and mortality…\n   In the plots, the color yellow refers to a year closer to 1990 where purple is more recent towards 2014. Based on this we can see the relative changes over time on the estimated TB prevalence, incidence and mortality. Let’s focus on the prevalence and incidence plots – we can see most of the countries have lower estimated TB prevalences over time. The most drastic decreases can be seen in Central African Republic and Niger. We can also see that Lesotho, South Africa and Swaziland have increased estimated TB prevalence during the 1990-2014 time period. Namibia is an interesting one because it seems that the estimated TB prevalence increased then decreased roughly half way. To accurately visualize this trend in Namibia, we would need a choose a different method to better visualize this (a simple line plot).\nRemember these plots are just a simple qualitative way to compare trends but doesn’t capture the full picture. I’ve learned that your method of visualizing data depends on which narrative of the data you want to highlight. These graphics could be used to see which countries have had the largest changes over time or as an example, identify countries who have improved (or failed) their national TB control, using estimated TB incidence as a proxy. Are TB control or prevention systems working in these countries? Based on allocated resources or programs, are the changes in line with the goals to reduce the global burden of TB.\nYou can tinker with the code to select which countries or regions you want to compare. I only displayed the plots for the WHO Africa Region but the other regions also looked interesting. You can adapt this code for the other WHO regions to see what the trends look like.\n Closing Thoughts I hope this post was informative and you learned something about TB around the world. Despite this disease being preventable and curable, it remains a real threat for many people around the world. There are groups fighting TB around the world, just one close to Boston is the endTB partnership. Check out their website and see the groups (PIH, MSF) involved in the work of combating MDR-TB.\nAs always I’m open to comments and suggestions on improving my code and graphics. If anyone has questions related to the code – send me a message, I’ll be more than happy to help. I’ve learned so much from online resources and communities like StackOverflow and would always want to help those new to using R.\n ","date":1488499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586625311,"objectID":"6d1d2291b82a6b639952420f3fef5d67","permalink":"/post/who-tb-data-ggplot2/","publishdate":"2017-03-03T00:00:00Z","relpermalink":"/post/who-tb-data-ggplot2/","section":"post","summary":"Original post date: March 03 2017\nFor this post, as similar to previous ones, I give a guide through the process I took to organize the data, a nifty function I created to search the documentation and a visualization of the data using ggplot2.\nWhat is Tuberculosis? Tuberculosis (TB) is a nasty disease caused by the bacterium Mycobacterium tuberculosis. It was first described by Hippocrates and has a history concurrent with human civilization.","tags":[],"title":"WHO Tuberculosis Data \u0026 ggplot2","type":"post"},{"authors":[],"categories":["R","Refugees"],"content":" Original post date: February 02 2017\nThe new United States presidential administration formulated an executive order the end of last week, if issued, will suspend visas of internationals in the United States (either for school or work) from select countries, prevent the the entrance of Syrian refugees indefinitely and a “readjustment” of the the Refugee and Immigration Programs. Because of this disruptive change in immigration – especially for the most vulnerable populations that include refugees, asylum seekers and displaced people, all fleeing from conflict, instability or persecution – I wanted create a post on this topic.\nWhere’s the Data? The data I’m using is taken from the United Nations High Commissioner for Refugees (UNHCR) website – the UN Refugee Agency. You can read more on what they do and why the exist in the link above.\nIn this post I am not going to share my views on the current political landscape in the United States (that can be a complete separate blog on its own). Similar to my previous posts, I’m going to do a walkthrough with data cleaning, ask a couple questions and visualize the data in some meaningful way. If you want to skip the details on the code, you can just scroll down to the figures.\n Working in R As usual, all the code is in my GitHub repository for whoever wants to download and use it for themselves. For this post, I’m going to use the following packages ggplot2, grid, scales, reshape2, and worldcloud. I was reading some R manuals and I discovered a “new”\u0026quot; way to access your working directory environment. It involves the use of the list.files() function, which lists the files or folders in your current working directory. You can also further specify the path name (which saves the time of constantly changing the pathname in the setwd() function (which I have been foolishly doing for a while).\nsetwd(\u0026quot;~/Documents/UNHCR Data/\u0026quot;) # ~ acts as base for home directory list.files(full.names=TRUE) # gives full path for files or folders files.all \u0026lt;- list.files(path=\u0026quot;All_Data/\u0026quot;) #assigns name to file names in the All_Data folder length(files.all) #checks how many objects there are in the /All_Data folder files.all Since the file is a comm delimited file (.csv) we use the read.csv() function to read it in and assign it the name “ref.d”. I used what I know with the paste0() function We set the skip argument equal to 2 because by visual inspection of the file, the first two rows are committed to the file title (we don’t want to read that into R). I also used the na.string argument to specify that any blanks (“”), dashes (“-”) and asterisks (“*“) would be considered missing data, NA. The asterisks are specified to be redacted information, based on the UNHCR website.\nref.d \u0026lt;- read.csv(paste0(\u0026quot;All_Data/\u0026quot;,files.all), #insert filepath and name into 1st argument header=T, #select the headers in 3rd row skip=2, #skips the first rows (metadata in .csv file) na.string=c(\u0026quot;\u0026quot;,\u0026quot;-\u0026quot;,\u0026quot;*\u0026quot;), #convert all blanks, \u0026quot;i\u0026quot;,\u0026quot;*\u0026quot; cells into missing type NA col.names=new.names #since we already made new names ) First thing I see, the names for the columns names are long and they would be annoying to reproduce. So first we’ll change these using the names() function.\nnew.names \u0026lt;- c(\u0026quot;Year\u0026quot;, \u0026quot;Country\u0026quot;, \u0026quot;Country_Origin\u0026quot;, \u0026quot;Refugees\u0026quot;, \u0026quot;Asylum_Seekers\u0026quot;, \u0026quot;Returned_Refugees\u0026quot;, \u0026quot;IDPs\u0026quot;, \u0026quot;Returned_IDPs\u0026quot;, \u0026quot;Stateless_People\u0026quot;, \u0026quot;Others_of_Concern\u0026quot;,\u0026quot;Total\u0026quot;) names(ref.d) \u0026lt;- new.names By using summary() and str() on the dataset we can see that the range of the data spans from 1951 to 2014, it contains information on the country where refugees are situated, their country of origin, counts for each population of concern and total counts.\nsummary(ref.d) # Year Country Country_Origin Refugees # Min. :1951 Length:103746 Length:103746 Min. : 1 # 1st Qu.:2000 Class :character Class :character 1st Qu.: 3 # Median :2006 Mode :character Mode :character Median : 16 # Mean :2004 Mean : 5992 # 3rd Qu.:2010 3rd Qu.: 167 # Max. :2014 Max. :3272290 # NA\u0026#39;s :19353 # Asylum_Seekers Returned_Refugees IDPs Returned_IDPs # Min. : 0.0 Min. : 1 Min. : 470 Min. : 23 # 1st Qu.: 1.0 1st Qu.: 2 1st Qu.: 90746 1st Qu.: 5000 # Median : 5.0 Median : 16 Median : 261704 Median : 27284 # Mean : 255.9 Mean : 6737 Mean : 540579 Mean : 111224 # 3rd Qu.: 34.0 3rd Qu.: 247 3rd Qu.: 594443 3rd Qu.: 104230 # Max. :358056.0 Max. :9799410 Max. :7632500 Max. :1186889 # NA\u0026#39;s :46690 NA\u0026#39;s :97327 NA\u0026#39;s :103330 NA\u0026#39;s :103544 # Stateless_People Others_of_Concern Total # Min. : 1 Min. : 1.0 Min. : 1 # 1st Qu.: 205 1st Qu.: 14.8 1st Qu.: 3 # Median : 1720 Median : 444.5 Median : 16 # Mean : 66803 Mean : 24672.5 Mean : 8605 # 3rd Qu.: 11462 3rd Qu.: 6000.0 3rd Qu.: 166 # Max. :3500000 Max. :957000.0 Max. :9799410 # NA\u0026#39;s :103103 NA\u0026#39;s :103018 NA\u0026#39;s :2433 It’s always good practice to identify missing data as well (especially when we set the condition of the read.csv argument above). For non-numeric variables you can use a simple function using apply() and is.na() to identify missing values or NA in your data.\napply(ref.d,2, function(x) sum(is.na(x))) When I used str() on the data, I saw that the country names were as factors and the populations of concern categories were integers. I made a short for loop to change these to a character and numeric type respectively.\n# \u0026quot;2\u0026quot;-ignores the first column (we want to keep Year as an integer) for(i in 2:length(names(ref.d))){ if (class(ref.d[,i])==\u0026quot;factor\u0026quot;){ ref.d[,i] \u0026lt;- as.character(ref.d[,i])} if (class(ref.d[,i])==\u0026quot;integer\u0026quot;){ ref.d[,i] \u0026lt;- as.numeric(ref.d[,i])} } Also another nuance, I wanted to change some of the names of the countries (they were either very long or had extra information). I first identified the names I wanted to change and then replace them a new set of names. I did this using a for loop as well.\nold.countries \u0026lt;- c(\u0026quot;Bolivia (Plurinational State of)\u0026quot;, \u0026quot;China, Hong Kong SAR\u0026quot;, \u0026quot;China, Macao SAR\u0026quot;, \u0026quot;Iran (Islamic Rep. of)\u0026quot;, \u0026quot;Micronesia (Federated States of)\u0026quot;, \u0026quot;Serbia and Kosovo (S/RES/1244 (1999))\u0026quot;, \u0026quot;Venezuela (Bolivarian Republic of)\u0026quot;, \u0026quot;Various/Unknown\u0026quot;) # replacement names new.countries \u0026lt;- c(\u0026quot;Bolivia\u0026quot;,\u0026quot;Hong Kong\u0026quot;,\u0026quot;Macao\u0026quot;,\u0026quot;Iran\u0026quot;,\u0026quot;Micronesia\u0026quot;,\u0026quot;Serbia \u0026amp;amp;amp; Kosovo\u0026quot;,\u0026quot;Venezuela\u0026quot;,\u0026quot;Unknown\u0026quot;) for (k in 1:length(old.countries)){ ref.d$Country_Origin[ref.d$Country_Origin==old.countries[k]]\u0026amp;amp;lt;-new.countries[k] ref.d$Country[ref.d$Country==old.countries[k]]\u0026amp;amp;lt;-new.countries[k] } If any has alternative ways to achieve the above (ie. using the apply family), comment below! Just a short disclaimer on for loops in R. There has been a lot of argument on the effectiveness of for loops in R compared to the apply function family. A quick Google Search shows many opinions on this issue, based on computing speed/power, simplicity, elegance, etc. Advanced R by Hadley Wickham talks about this and I’ve generally used this as a guideline on whether to use a for loop or an apply function.\n Some Descriptives and North Korea Just to get an idea of the data, we can create a list of the countries and countries of origin\nclist\u0026lt;-sort(unique(ref.d$Country)) #alphabetical clist or.clist\u0026lt;-sort(unique(ref.d$Country_Origin)) #alphabetical or.clist We can then compare them for any differences either using matching operators or the setdiff() function. First we’ll do this…\nclist[!clist %in% or.clist] # or setdiff(clist,or.clist) # [1] \u0026quot;Bonaire\u0026quot; \u0026quot;Montserrat\u0026quot; # [3] \u0026quot;Sint Maarten (Dutch part)\u0026quot; \u0026quot;State of Palestine\u0026quot; … we can infer that these countries haven’t produced refugees or there is no data on these countries in the UNHCR database. If we reverse the comparison…\nor.clist[!or.clist %in% clist] # or setdiff(or.clist,clist) # [1] \u0026quot;Andorra\u0026quot; \u0026quot;Anguilla\u0026quot; # [3] \u0026quot;Bermuda\u0026quot; \u0026quot;Cook Islands\u0026quot; # [5] \u0026quot;Dem. People\u0026#39;s Rep. of Korea\u0026quot; \u0026quot;Dominica\u0026quot; # [7] \u0026quot;French Polynesia\u0026quot; \u0026quot;Gibraltar\u0026quot; # [9] \u0026quot;Guadeloupe\u0026quot; \u0026quot;Holy See (the)\u0026quot; # [11] \u0026quot;Kiribati\u0026quot; \u0026quot;Maldives\u0026quot; # [13] \u0026quot;Marshall Islands\u0026quot; \u0026quot;Martinique\u0026quot; # [15] \u0026quot;New Caledonia\u0026quot; \u0026quot;Niue\u0026quot; # [17] \u0026quot;Norfolk Island\u0026quot; \u0026quot;Palestinian\u0026quot; # [19] \u0026quot;Puerto Rico\u0026quot; \u0026quot;Samoa\u0026quot; # [21] \u0026quot;San Marino\u0026quot; \u0026quot;Sao Tome and Principe\u0026quot; # [23] \u0026quot;Seychelles\u0026quot; \u0026quot;Stateless\u0026quot; # [25] \u0026quot;Tibetan\u0026quot; \u0026quot;Tuvalu\u0026quot; # [27] \u0026quot;Wallis and Futuna Islands \u0026quot; \u0026quot;Western Sahara\u0026quot; … we get a list of countries that have only produced refugees (not taken any refugees in) or there is missing data in the UNHCR. I myself being Korean-Canadian I noticed North Korea (the Democratic People’s Republic of Korea) on this list. I wanted to ask the question, which countries have the largest number of North Korean refugees based on the UNHCR Data? What are the top 10?\nNK.tot\u0026lt;- aggregate(cbind(Total)~Country,data=NK,FUN=sum) NK.tot[order(-NK.tot[,2]),][1:10,] # Country Total # 31 United Kingdom 4808 # 5 Canada 2954 # 10 Germany 2845 # 18 Netherlands 487 # 3 Belgium 435 # 23 Russian Federation 357 # 32 United States of America 346 # 1 Australia 318 # 20 Norway 262 # 9 France 228 We find that the UK has the highest number of North Koreans refugees, followed by Canada and Germany with similar counts. Learned something new today.\n Word Clouds in R If you haven’t guessed based on the packages I loaded in the beginning, a word cloud was inevitable. Making word clouds in R is pretty simple thanks to the wordcloud package. Remember that word clouds are an esthetically decent way to qualitatively see your data. There are bunch of pros and cons on using word clouds. Generally this is appropriate if you just want see the general relative frequency of words in your data. Making any quantitative conclusions would be erroneous.\nFirst we’ll aggregate the counts for the country of origin.\nor.count.tot \u0026lt;- aggregate(cbind(Total)~Country_Origin,data=ref.d,FUN=sum) or.count.tot Then set color palette using HEX codes, much thanks to I Want Hue.\npal3 \u0026lt;- c(\u0026quot;#274c56\u0026quot;, \u0026quot;#664c47\u0026quot;, \u0026quot;#4e5c48\u0026quot;, \u0026quot;#595668\u0026quot;, \u0026quot;#395e68\u0026quot;, \u0026quot;#516964\u0026quot;, \u0026quot;#6b6454\u0026quot;, \u0026quot;#58737f\u0026quot;, \u0026quot;#846b6b\u0026quot;, \u0026quot;#807288\u0026quot;, \u0026quot;#758997\u0026quot;, \u0026quot;#7e9283\u0026quot;, \u0026quot;#a79486\u0026quot;, \u0026quot;#aa95a2\u0026quot;, \u0026quot;#8ba7b4\u0026quot;) Then let’s make a word cloud based on the countries of origin for the populations of concern in the data and save it as a .png file. I’ve commented the code for each line We can see that Afghanistan seems to have the highest relative number and those with unknown countries of origin is second. Other high conflict countries are visible too like DRC, Iraq, Sudan, Syria, Somalia, etc.\n# COUNTRY OF ORIGIN png(\u0026quot;wordcloud_count_or.png\u0026quot;,width=600,height=850,res=200) wordcloud(or.count.tot[,1], #list of words or.count.tot[,2], #frequencies for words scale=c(3,.5), #scale of size range min.freq=100, #minimum frequency max.words=100, #maximum number of words show (others dropped) family=\u0026quot;Garamond\u0026quot;, font=2, #text edit (The font \u0026quot;face\u0026quot; (1=plain, 2=bold, 3=italic, 4=bold-italic)) random.order=F, #F-plotted in decreasing frequency colors=rev(pal3)) #colours from least to most frequent (reverse order) dev.off()  Historical Data Visualized So in the UNHCR data, it contains information not only on countries but based on the year from 1951 to 2014. I wanted to see the trends over time between each of the populations of concern. There are seven types of populations in the dataset. Their definitions can be found here on the UNHCR website. To visualize the data using ggplot2, it’s a good practice to convert the data from wide to long form. The benefits on using long form can be read here and here. The accomplish this, we’re going to use the reshape2 package and the melt() function. This tutorial is helpful in understanding what’s going on.\nI first subset to remove the country, country of origin and total columns.\nno.count\u0026lt;-(ref.d[c(-2,-3,-11)]) #removes Country, Country of Origin and Total lno.count\u0026lt;-melt(no.count,id=c(\u0026quot;Year\u0026quot;)) head(lno.count) # Year variable value # 1 1951 Refugees 180000 # 2 1951 Refugees 282000 # 3 1951 Refugees 55000 # 4 1951 Refugees 168511 # 5 1951 Refugees 10000 # 6 1951 Refugees 265000 Then I set the ggplot2 theme and the color palette I want to use.\nblank_t \u0026lt;- theme_minimal()+ theme( panel.border = element_blank(), #removes border panel.background = element_rect(fill = \u0026quot;#d0d1cf\u0026quot;,colour=NA), panel.grid = element_line(colour = \u0026quot;#ffffff\u0026quot;), plot.title=element_text(size=20, face=\u0026quot;bold\u0026quot;,hjust=0,vjust=2), #set the plot title plot.subtitle=element_text(size=15,face=c(\u0026quot;bold\u0026quot;,\u0026quot;italic\u0026quot;),hjust=0.01), #set subtitle legend.title=element_text(size=10,face=\u0026quot;bold\u0026quot;,vjust=0.5), #specify legend title axis.text.x = element_text(size=9,angle = 0, hjust = 0.5,vjust=1,margin=margin(r=10)), axis.text.y = element_text(size=10,margin = margin(r=5)), legend.background = element_rect(fill=\u0026quot;gray90\u0026quot;, size=.5), legend.position=c(0.15,0.65), plot.margin=unit(c(t=1,r=1.2,b=1.2,l=1),\u0026quot;cm\u0026quot;) ) pal5 \u0026lt;- c(\u0026quot;#B53C26\u0026quot;, #Refugees 7 TOTAL \u0026quot;#79542D\u0026quot;, #Asylum Seekers \u0026quot;#DA634D\u0026quot;, #Returned Refugees \u0026quot;#0B4659\u0026quot;, #IDPs \u0026quot;#4B8699\u0026quot;, #Returned IDPs \u0026quot;#D38F47\u0026quot;, #Stateless People \u0026quot;#09692A\u0026quot;) #Others of Concern Then use geom_bar() and the fill argument to make the stacked bar graph over time by specifying what titles and axis labels. In the scale_fill_manual() I used the gsub() function remove the underscores from the names (I was lazy to rewrite the names out in a vector and assign it to the labels).\ngg1 \u0026lt;-ggplot(lno.count,aes(Year,value)) + geom_bar(aes(fill=variable),stat=\u0026quot;identity\u0026quot;) + labs(title=\u0026quot;UNHCR Population Statistics Database\u0026quot;, subtitle=\u0026quot;(1951 - 2014)\u0026quot;, x=\u0026quot;Year\u0026quot;,y=\u0026quot;Number of People (Millions)\u0026quot;) + blank_t + scale_fill_manual(guide_legend(title=\u0026quot;Populations of Concern\u0026quot;),labels=gsub(\u0026quot;*_\u0026quot;,\u0026quot; \u0026quot;,names(ref.d)[c(-1,-2,-3,-11)]),values=pal5) Next while modifying the axes, I wanted to change the scale of the y-axis to shows it as millions versus 6 zeros. I set the labels argument in scale_y_continuous() to a short function that converts it by a factor of 10e-6. The plot is below and you can click on it to see the full-sized version.\nmil.func \u0026lt;- function(x) {x/1000000} gg2 \u0026lt;-gg1+ scale_y_continuous(limits=c(0,6e7), breaks=pretty(0:6e7,n=5), labels=mil.func, expand=c(0.025,0)) + scale_x_continuous(limits=c(1950,2015), breaks=seq(1950,2015,by=5), expand=c(0.01,0)) ggsave(plot=gg2,filename=\u0026quot;UNHCR_Totals_Yr.png\u0026quot;, width=9.5, height=5.5, dpi=200) So based on this, we can see that there has been a large increase of the total populations of concern the past 30 years. Internally Displaced People (IDPs) have also increased dramatically in the past 20 years. Based on the most recent data, the totals are reaching 65.3 million! (that’s above the y-limit in the bar graph)\n Let’s Be Real (Not R related) Based on doing this, I learned that the UK hosts the highest number of North Korean refugees in the world, that Afghanistan has historically produced the largest number of refugees and there is a upward trend (qualitative conclusion) with the total number of displaced people in recent history. Remembering that these vulnerable populations are mostly undocumented so the numbers in this dataset are likely underestimates to the actual numbers of people who fall under the population of concern definition. In the midst of the news and social media firestorm on all the unfortunate events happening in the world, I think it’s easy to dissociate and just live our own lives. I believe that those with access to resources should be generous with how we use them. A simple act of kindness like a Samaritan can go a long way. Some simple steps that I’ve taken were to educate myself on these issues, advocate where I am able, donate financially and engage where I can. There are a plethora of organizations committed to relieving the burden of these populations – one of them I support is the International Rescue Committee.\nSo I hope this post was informative and demonstrates how one can pull some data off the internet, tinker around with R and discover some news things about the world. Any feedback on the code, alternative ways on what I did are always welcome!\n ","date":1485993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586625311,"objectID":"60c32a98a52f1c598639b7d9d51dda70","permalink":"/post/unhcr-viz/","publishdate":"2017-02-02T00:00:00Z","relpermalink":"/post/unhcr-viz/","section":"post","summary":"Original post date: February 02 2017\nThe new United States presidential administration formulated an executive order the end of last week, if issued, will suspend visas of internationals in the United States (either for school or work) from select countries, prevent the the entrance of Syrian refugees indefinitely and a “readjustment” of the the Refugee and Immigration Programs. Because of this disruptive change in immigration – especially for the most vulnerable populations that include refugees, asylum seekers and displaced people, all fleeing from conflict, instability or persecution – I wanted create a post on this topic.","tags":[],"title":"UNHCR Refugee Data Visualized","type":"post"},{"authors":[],"categories":["R","Public Health"],"content":" Original post date: January 24 2017\nDuring one of my summers, I had the opportunity to conduct some research on the prevalence of methicillin-resistant Staphylococcus aureus (MRSA) in vulnerable populations and examining US emergency department data and I thought this would be a pretty interesting topic to expand on for my thesis in light of the increasing concerns of antimicrobial resistance, both at the domestic and international level. This resulted in an systematic review of the recent literature of MRSA in the major Far East countries (China, Hong Kong, Japan, South Korea and Taiwan) which has a wide variety of genotypes and strains. If you’re interested in understanding S. aureus in Asia, a good starting point is this article. An extensive and great review on community-associated MRSA written by two researchers from Chicago if you really want to dig deep into this topic.\nWhat is MRSA? MRSA is a gram-positive bacteria which is found on almost every continent on earth, which developed resistance to methicillin (a narrow spectrum β-lactam antibiotic from the penicillin class) in the early 60s. MRSA was originally considered to only cause infections within hospital settings (ICUs, EDs, surgery) but towards the end of the 1980s and into the 1990s, cases of severe MRSA infections were being reported in communities outside of healthcare centers. MRSA typically causes skin and soft tissue infections but some cases when left untreated can cause havoc to respiratory system, bloodstream and eventually cause sepsis – not a pretty picture. Over the next 20 years, reports of different strains of this “new” MRSA started to appear around the globe. Some of these strains successfully disseminated from their country of origin to neighboring regions and even across continents with the help of globalization. With the advent and advancement of genotyping technology – researchers were able to track and identify the strains causing outbreaks across the world. One classification system is called Multi Locus Sequence Typing (MLST) which was developed by researchers at Imperial College, UK. In short, it is based on creating a profile or sequence type (ST) of each S. aureus strain based on sequencing specific internal nucleotide fragments of seven house-keeping genes. Using this method, investigators are able to discriminate between strains and identify evolutionary changes over time.\n Reading in the data using R This post has nothing too fancy in R – I’m just going to do a step-by-step walkthrough of data cleaning and asking a couple questions, then running descriptive analyses to find some answers. For those you are proficient in doing this might find this a little boring (you can just scroll down to the end and see the results). The data is taken from https://pubmlst.org/saureus/ by selecting the link “download as MS excel” and downloading the spreadsheet to a local directory. The data is complied from submissions to the database which report new MLSTs with some demographic information. This database doesn’t really provide information of a specific strain’s prevalence – it’s just a timestamp report for novel strains. This file contains nine sheets. The one using for this post is called “profile”. I saved this as a comma-delimited file manually but there are lots of resources on the internet of doing this in R using some packages.\nSo now you should have a .csv file someone on your local directory. You first want to set your working directory. You can read a brief description of how to do this here. I’m using R 3.3.2 on Mac OS as an FYI, the link before talks about the differences when you set your working directory in Windows and Mac OS.\ngetwd() #shows what your current working directory is setwd(\u0026quot;/Users/myname/Documents/Folder\u0026quot;) #sets your working directory Next, we’re gonna read the comma delimited file into R using the read.csv() function and assigning it the name “profile”. The first argument you want input the filename with the .csv extension. The second argument tells the function that the first row in the file contains the headers or column names. Based on a quick visual inspection of spreadsheet file using Excel, there are many missing values just left as blank cells. The final argument I used specifies to convert any blank cells to NA type so that R can just treat these as missing data.\nprofile \u0026lt;- read.csv(\u0026quot;name_of_your_file.csv\u0026quot;, header=T, na.strings=c(\u0026quot;\u0026quot;, NA)) A good practice I always do when I read in spreadsheets into R is to get a idea of what I’m dealing with. I want to know how many columns and rows there are, the names of the headers/columns and the type of data in each column. To accomplish this, we run the dim(), names() and str() functions for each respective question.\ndim(profile) #dimensions of the dataset names(profile) # names of the columns in dataset str(profile) #structure of the data frame When we do this, the data frame should have 4703 rows and 25 columns. The 25 headers include the following…\n# [1] \u0026quot;id\u0026quot; \u0026quot;strain\u0026quot; \u0026quot;other_name\u0026quot; \u0026quot;st\u0026quot; \u0026quot;country\u0026quot; # [6] \u0026quot;region\u0026quot; \u0026quot;year\u0026quot; \u0026quot;age_yr\u0026quot; \u0026quot;age_mth\u0026quot; \u0026quot;sex\u0026quot; # [11] \u0026quot;disease\u0026quot; \u0026quot;sourcce\u0026quot; \u0026quot;epidemiology\u0026quot; \u0026quot;species\u0026quot; \u0026quot;oxsau\u0026quot; # [16] \u0026quot;methicillin\u0026quot; \u0026quot;vancomycin\u0026quot; \u0026quot;spa_type\u0026quot; \u0026quot;reference\u0026quot; \u0026quot;comments\u0026quot; # [21] \u0026quot;sender\u0026quot; \u0026quot;curator\u0026quot; \u0026quot;date_entered\u0026quot; \u0026quot;datestamp\u0026quot; \u0026quot;username\u0026quot; Most of the time the columns names can be cryptic. You always want to have the metadata or the documentation files for the datasets so that you know exactly what each column represents. In this case for this file, it is pretty straightforward – we can see that for each submission there is an id number, strain name, sequence type (st), who submitted the info, time/date stamps and some other demographic information.\nWhen we run str(profile), our output shows the class type of each column. This will be important as R tends to have functions that run only for specific types of data. A good overview the different types of data in R can be found here.\nTwo major things stick out to me when I look through the data – first the column with country names is type factor. I want to convert into a string type, so later on I can subset the data by country by referring to the actual name, versus the factor level (which would be confusing and annoying to keep track of).\n # Change country to character from factor type profile$country \u0026lt;- as.character(profile$country) Second, I noticed the classification of a lot of categorical columns have multiple levels. Sex has 6 levels!? Methicillin resistance has 9 levels? What does this mean…? Well, the problem for databases that are derived from public submissions is that there is no standardization with the data input. When we inspect what the levels are for sex by running the line of code below, we see that due to the lack of data input standardization, we have F, female and Female for those categorized as female sex. For male sex we have M and Male, and then Unspecified for those contributors who didn’t leave it blank.\n# \u0026gt; summary(profile$sex) # F female Female M Male Unspecified NA\u0026#39;s # 17 1 814 19 842 1343 1667 I’m not particularly interested in distribution of gender with these reports so I’m okay with leaving this alone. But we have to same issue with the column for methicillin resistance and I want to simplify it to a binary category of resistant and susceptible. When we look at the levels for the the methicillin column we see this…\n# \u0026gt; summary(profile$methicillin) # I MIC 4mg/L MIC 64mg/L r R s S # 1 2 1 1 1533 3 2175 # Unknown Unspecified NA\u0026#39;s # 4 199 784 Since we want to simplify this, we can use the levels() function to set and rename the levels in this column. First, we’re gonna merge the lower and uppercase letters together (r will be R and s will be S)\nlevels(profile$methicillin) \u0026lt;- c(\u0026quot;I\u0026quot;, \u0026quot;MIC4\u0026quot;, \u0026quot;MIC64\u0026quot;, \u0026quot;R\u0026quot;, \u0026quot;R\u0026quot;, \u0026quot;S\u0026quot;, \u0026quot;S\u0026quot;, \u0026quot;Unknown\u0026quot;, \u0026quot;Unspecified\u0026quot;) Minimum Inhibitory Concentration (MIC) is the lowest concentration of an antibiotic that inhibits the growth of an organism. If you have two strains of S. aureus (#1 and #2) where #1 has a higher MIC to methicillin or oxacillin (another antibiotic) compared to strain #2 strain, #1 is considered to be more resistant to that tested antibiotic compared to #2. This is a method used to provide more granularity in defining antimicrobial resistance. Here in this case, I recognize MIC’s above 4mg/L are considered resistant (I’m making an assumption that all the reports used the same standard and technique to make this definition, a limitation is simplifying this). There’s more information on MIC’s for MRSA on the CDC’s website. So with all this in mind, I going to merge the MIC4 and MIC64 into the resistant category. After doing this, we only have four levels in the methicillin column: R, S, Unknown and Unspecified. We can check this by running\nlevels(profile$methicillin) \u0026lt;- c(\u0026quot;R\u0026quot;,\u0026quot;R\u0026quot;,\u0026quot;R\u0026quot;,\u0026quot;R\u0026quot;,\u0026quot;S\u0026quot;,\u0026quot;Unknown\u0026quot;,\u0026quot;Unspecified\u0026quot;) factor(profile$methicillin) Before we move on, another good practice is to be aware (and quantify) missing values in each of the columns. Lets say we want to know how many missing sequence types (STs) are there? We can use the is.na() and which() together to figure out which rows are missing values under the st column.\n# \u0026gt; which(is.na(profile$st)) # [1] 1596 1897 4602 4605 Using this, we now know that there are four rows missing STs. Row numbers 1596, 1897, 4602 and 4605. If we wanted to subset the data for those missing STs, we can just use square brackets [...] to accomplish this. If you’re unfamiliar with this, you can read this post to learn more on these accessors.\nprofile[which(is.na(profile$st)),]  Let’s ask some questions Now that we’re done with the appropriate cleaning of the data, let’s ask some (random) questions\n1. Which countries reported sequence type 239 (ST-239)? Based on my literature review, ST-239 is a common hospital-associated strain in Asia and it’s prevalence has increased in China, where is it the most common strain in the entire country. Other variants of ST-239 are common in Eastern Europe and South America as well, in particular Hungary and Brazil respectively.\nTo answer this question, we use the logic statements with subsetting to accomplish this.\nid.239 \u0026lt;- which(profile$st==239) #index of rows with ST-239 st.239 \u0026lt;-profile[id.239,] #new dataframe of only ST-239 sort(unique(st.239$country)) # alphabetical list of countries with ST-239 When we run the sort() function we get a vector of 29 countries in alphabetical order which have reported ST-239.\n# [1] \u0026quot;Algeria\u0026quot; \u0026quot;Australia\u0026quot; \u0026quot;Brazil\u0026quot; # [4] \u0026quot;China\u0026quot; \u0026quot;Czech republic\u0026quot; \u0026quot;Eire\u0026quot; # [7] \u0026quot;Finland\u0026quot; \u0026quot;Germany\u0026quot; \u0026quot;Greece\u0026quot; # [10] \u0026quot;Hungary\u0026quot; \u0026quot;India\u0026quot; \u0026quot;Iran\u0026quot; # [13] \u0026quot;Malaysia\u0026quot; \u0026quot;Pakistan\u0026quot; \u0026quot;Poland\u0026quot; # [16] \u0026quot;Portugal\u0026quot; \u0026quot;Scotland\u0026quot; \u0026quot;Slovenia\u0026quot; # [19] \u0026quot;South Africa\u0026quot; \u0026quot;Spain\u0026quot; \u0026quot;Sweden\u0026quot; # [22] \u0026quot;Switzerland\u0026quot; \u0026quot;Taiwan\u0026quot; \u0026quot;Thailand\u0026quot; # [25] \u0026quot;The Netherlands\u0026quot; \u0026quot;Turkey\u0026quot; \u0026quot;UK\u0026quot; # [28] \u0026quot;United Arab Emirates\u0026quot; \u0026quot;USA\u0026quot; 2. Which countries reported ST8 and spa type t008? spa typing is another classification technique used for S. aureus strains. It is a single-locus typing method, so it’s much more cost-effective compared to MLST.\nWe use the similar code as above except we add another condition to the logic statement to specify the spa type.\nid.ST8.t008 \u0026lt;- which(profile$st==8 \u0026amp; profile$spa_type==\u0026quot;t008\u0026quot;) ST8.t008 \u0026lt;- profile[id.ST8.t008,] sort(unique(ST8.t008$country)) We see that 4 countries that have reported ST-8 with spa type t008.\n# [1] \u0026quot;Algeria\u0026quot; \u0026quot;France\u0026quot; \u0026quot;Portugal\u0026quot; \u0026quot;Switzerland\u0026quot; 3. What is the proportion of methicillin-susceptible vs. -resistant S. aureus reports are there in North America? I was honestly just curious, the results we get from this are definitely limited due to the assumptions we had with the definitions and simplification discussed earlier.\nFirst, since there is no column specifying the region of North America, we have to specify which countries to include to this subset. By visual inspection of the countries using sort(unique(profile$country)), we see that there Canada and USA but no Mexico. So we’ll subset the data to only Canada and USA.\n#selection of North American countries NorthA.MRSA \u0026lt;- profile[profile$country==\u0026quot;USA\u0026quot; | profile$country==\u0026quot;Canada\u0026quot;,]  Then running the following line, we get the counts of methicillin-resistant and methicillin-susceptible in North America.\n# table(NorthA.MRSA$methicillin,useNA=\u0026quot;always\u0026quot;) # R S Unknown Unspecified \u0026lt;NA\u0026gt; # 138 148 0 15 75 4. How many different STs are there in China, S Korea, Taiwan and Japan? This is just coming from my thesis and getting an idea of distributions in the region of the world.\nF.east \u0026lt;- profile[profile$country==\u0026quot;China\u0026quot; | profile$country==\u0026quot;South Korea\u0026quot; | profile$country==\u0026quot;Japan\u0026quot; | profile$country==\u0026quot;Taiwan\u0026quot;,] The following lines of code will answer some other questions related to this…\nfactor(F.east$st) #find the number of STs in all these countries sum(is.na(F.east$st)) #number of missing values table(F.east$country) #counts of STs by country Based on 573 entries, we find that 289 different STs; China has 241, Japan has 259, South Korea has 26 and Taiwan 59.\nIf we want to only look those without missing values…\n#removal of all rows that are missing an entry for country FE.final\u0026lt;-F.east[complete.cases(F.east$country),]  To go further we can look at each specific country, lets ask what are the top 5 most reported STs in China?\n# FE.China \u0026lt;- FE.final[FE.final$country==\u0026quot;China\u0026quot;,] #subset for China # sort(table(FE.China$st),decreasing=T)[1:5] #top 5 most reported # # 97 239 398 88 2154 # 21 13 5 3 3 5. What are the 10 most frequently STs reported in North America in this database? Let’s also visualize this in a basic pie chart? First need to setup the data so that it is easier to visualize using the ggplot2 package. So we setup the STs for North America and then order the the counts/frequencies of STs. *note you can also accomplish this using the count() function in plyr package\nc.NorthA.MRSA \u0026lt;-as.data.frame(table(NorthA.MRSA$st)) names(c.NorthA.MRSA) \u0026lt;- c(\u0026quot;ST\u0026quot;,\u0026quot;count\u0026quot;) #data frame containing count frequencies of the STs c.NorthA.MRSA\u0026lt;-c.NorthA.MRSA[order(-c.NorthA.MRSA$count),] #order the STs by count frequencies c.NorthA.MRSA[1:10,] # top ten Then we use ggplot2 to create the pie chart! First specifying the theme so we can tinker around with font size, remove the background grid, etc. I used the scales package to display the percentages in the pie chart for each ST. I stumbled across this incredible resource for color palettes for data visualization where you can choose distinct colors (with color-blind friendly options as well!). This specific case I choose the pastel color option.\nWe use ggplot2 to create the pie chart! First specifying the theme so we can tinker around with font size, remove the background grid, etc. I used the scales package to display the percentages in the pie chart for each ST. I stumbled across this incredible resource for color palettes for data visualization where you can choose distinct colors (with color-blind friendly options as well!). This specific case I choose the pastel color option.\nblank_t \u0026lt;-theme( axis.title.x = element_blank(), axis.title.y = element_blank(), panel.border = element_blank(), panel.grid=element_blank(), axis.ticks = element_blank(), plot.title=element_text(size=20, face=\u0026quot;bold\u0026quot;,hjust=0.6), plot.subtitle=element_text(size=15,face=c(\u0026quot;bold\u0026quot;,\u0026quot;italic\u0026quot;),hjust=0.6), axis.text.x=element_blank(), legend.title=element_text(size=14,face=\u0026quot;bold\u0026quot;,vjust=0.5), panel.margin=unit(2,\u0026quot;cm\u0026quot;) ) pie1\u0026lt;-ggplot(c.NorthA.MRSA[1:10,], aes(x=\u0026quot;\u0026quot;,y=count,fill=ST,order=ST)) + geom_bar(width=1, stat=\u0026quot;identity\u0026quot;) + ggtitle(element_text(size=50))+ labs(title = \u0026quot;STs in North America\u0026quot;, subtitle = \u0026quot;Canada and United States\u0026quot;,y=NULL) + coord_polar(theta=\u0026quot;y\u0026quot;)+ geom_text(aes(label = scales::percent(count/sum(c.NorthA.MRSA[1:10,2]))),size=4, position = position_stack(vjust = 0.6)) + blank_t + scale_fill_manual(guide_legend(title=\u0026quot;STs\u0026quot;), values=c(\u0026quot;#e6b4c9\u0026quot;, \u0026quot;#bce5c2\u0026quot;, # color palette hexcodes \u0026quot;#c7b5de\u0026quot;, \u0026quot;#e3e4c5\u0026quot;, \u0026quot;#a1bbdd\u0026quot;, \u0026quot;#e5b6a5\u0026quot;, \u0026quot;#99d5e5\u0026quot;, \u0026quot;#bdbda0\u0026quot;, \u0026quot;#dcd1e1\u0026quot;, \u0026quot;#99c6b8\u0026quot;)) print(pie1) ggsave(plot=pie1,filename=\u0026quot;North_America_STs_MLSTdatabase.png\u0026quot;, width=5.75, height=7, dpi=120) Then we get the pie chart that looks like this… We can see that ST-5 and ST-8 have the most number of reports, while the remainder are ≤10%. All the code is on Github, open for anyone to use and adapt. I’m always open for suggestions to how to improve the methodology and code. I know pie charts are “controversial” so any input on other graphics to display this type of information is always welcome.\nHopefully this post demonstrates the power and flexibility of R when you have some questions and how easily you can find the answers in your data. It is always important to remember where the data came from, who collected it, when you received it, how you processed it and any assumptions you make along the way – this can change the interpretation for the results dramatically. The results from this specific dataset are just informative and descriptive of the MLST database and potentially lead to some hypotheses. Another possible extension of this data is attaching the strain information by country and displaying it in a map. There is a wealth of resources on mapping in R (the link is from the R-Bloggers website) and many packages exist for displaying spatial data – so this might be something I might tinker with in the near future.\n ","date":1485216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586625311,"objectID":"0ff29211d7a1bf5c5ffff93a11ef4196","permalink":"/post/mlst-mrsa/","publishdate":"2017-01-24T00:00:00Z","relpermalink":"/post/mlst-mrsa/","section":"post","summary":"Original post date: January 24 2017\nDuring one of my summers, I had the opportunity to conduct some research on the prevalence of methicillin-resistant Staphylococcus aureus (MRSA) in vulnerable populations and examining US emergency department data and I thought this would be a pretty interesting topic to expand on for my thesis in light of the increasing concerns of antimicrobial resistance, both at the domestic and international level. This resulted in an systematic review of the recent literature of MRSA in the major Far East countries (China, Hong Kong, Japan, South Korea and Taiwan) which has a wide variety of genotypes and strains.","tags":[],"title":"Descriptive Analysis of MLST Data for MRSA","type":"post"},{"authors":null,"categories":["R","Infectious Diseases"],"content":" Original post date: January 04 2017\nIn my infectious disease epidemiology course back in 2016, I first learned about a basic compartmental disease transmission model. I thought it was pretty awesome to predict and model how an infectious agent could affect a population. Purely out of interest I wanted to develop some working code in R to make a function that would plot the model based on specified parameters.\nThe basic deterministic model is composed of three compartments that represent different categories of individuals within a population; the susceptible, infected, and recovered – hence the SIR model. The relationship of these three groups is described by a set of differential equations first derived by Kermack and McKendrick. The SIR model details the transmission of infection through the contact of susceptible individuals with an infected host. If you are interested in learning more on this model, there is an online module.\nThe SIR Model  dS/dt = -βSI\ndI/dt = βSI – γI\ndR/dt = γI\nβ = cp  \nS – proportion of susceptible individuals in total population\nI – proportion of infected individuals in total population\nR – proportion of recovered individuals in total population\nβ – transmission parameter (rate of infection for susceptible-infected contact)\nc – number of contacts each host has per unit time (contact rate)\np – probability of transmission of infection per contact (transmissibility)\nγ – recovery parameter (rate of infected transitioning to recovered)\nThis model is very basic and has important assumptions. The first being the population is closed and fixed, in other words – no one it added into the susceptible group (no births), all individuals who transition from being infected to recovered are permanently resistant to infection and there are no deaths. Second, the population is homogenous (all individuals are the same) and only differ by their disease state. Third, infection and that individual’s “infectiveness” or ability to infect susceptible individuals, occurs simultaneously.\n Now on to using R! I used the deSolve package which was developed to solve the initial condition values of differential equations in R. The code to setup the SIR model was adapted from the MATLAB code from Modeling Infectious Diseases in Humans and Animals and an online demo in R.\nTo begin, I setup my R function to be dependent on the input of three parameters: time period (in days), and the values of β and γ. These are denoted by t,b,g within the function.\nThe initial conditions are set to have the proportion of the populationg being in the Susceptible group at \u0026gt;99.9% (1-1E-6 to be exact), the Infected group to be close to 0 (1E-6) and no one in the Recovered group. The SIR model is going to be plotted at 0.5 days for higher resolution.\nlibrary(deSolve) SIR.model \u0026lt;- function(t, b, g) { init \u0026lt;- c(S = 1 - 1e-6, I = 1e-6, R = 0) parameters \u0026lt;- c(bet = b, gamm = g) time \u0026lt;- seq(0, t, by = t / (2 * length(1:t))) } Next we setup the differential equation (from above) so that we can run the ode function from the deSolve package correctly.\neqn \u0026lt;- function(time, state, parameters) { with(as.list(c(state, parameters)), { dS \u0026lt;- -bet * S * I dI \u0026lt;- bet * S * I - gamm * I dR \u0026lt;- gamm * I return(list(c(dS, dI, dR))) }) } Then we run the ode function based on the parameters we set above and save coerce the output as a data frame class.\nout \u0026lt;- ode(y=init, times=time, eqn, parms=parameters) out.df \u0026lt;- as.data.frame(out) Now that we’ve solved the differential equation, the next task is to plot it. To do this I am going to be using the elegant visualization package ggplot2 (version 2.1.0).\nFirst I’m going to set a theme for the plot, by modifying one of the built-in ggplot2 themes, theme_bw. I referred to ggplot2 documentation found here.\nlibrary(ggplot2) mytheme4 \u0026lt;- theme_bw() + theme(text = element_text(colour = \u0026quot;black\u0026quot;)) + theme(panel.grid = element_line(colour = \u0026quot;white\u0026quot;)) + theme(panel.background = element_rect(fill = \u0026quot;#B2B2B2\u0026quot;)) theme_set(mytheme4) Here I set the title for the plot as SIR Model: Basic. For the subtitle it will show the values of β and γ set when first running the function. The use of bquote was based reading some material on R expressions, trial and error, with some scouring of Stack Overflow (which is an incredible resource FYI).\ntitle \u0026lt;- bquote(\u0026quot;SIR Model: Basic\u0026quot;) subtit \u0026lt;- bquote(list(beta==.(parameters[1]),~gamma==.(parameters[2]))) Now I describe the plot, essentially I am selecting the data frame from the ode function. Most of the code for theme and legend is just for aesthetics.\nres \u0026lt;- ggplot(out.df, aes(x = time)) + ggtitle(bquote(atop(bold(.( title )), atop(bold( .(subtit) ))))) + geom_line(aes(y = S, colour = \u0026quot;Susceptible\u0026quot;)) + geom_line(aes(y = I, colour = \u0026quot;Infected\u0026quot;)) + geom_line(aes(y = R, colour = \u0026quot;Recovered\u0026quot;)) + ylab(label = \u0026quot;Proportion\u0026quot;) + xlab(label = \u0026quot;Time (days)\u0026quot;) + theme(legend.justification = c(1, 0), legend.position = c(1, 0.5)) + theme( legend.title = element_text(size = 12, face = \u0026quot;bold\u0026quot;), legend.background = element_rect( fill = \u0026#39;#FFFFFF\u0026#39;, size = 0.5, linetype = \u0026quot;solid\u0026quot; ), legend.text = element_text(size = 10), legend.key = element_rect( colour = \u0026quot;#FFFFFF\u0026quot;, fill = \u0026#39;#C2C2C2\u0026#39;, size = 0.25, linetype = \u0026quot;solid\u0026quot; ) ) + scale_colour_manual( \u0026quot;Compartments\u0026quot;, breaks = c(\u0026quot;Susceptible\u0026quot;, \u0026quot;Infected\u0026quot;, \u0026quot;Recovered\u0026quot;), values = c(\u0026quot;blue\u0026quot;, \u0026quot;red\u0026quot;, \u0026quot;darkgreen\u0026quot;) ) So now the function runs, it prints the plot and write a .png image file to the file directory with the specified parameters in the filename. The ggsave function accomplishes this with ease.\nprint(res) ggsave( plot=res, filename=paste0(\u0026quot;SIRplot_\u0026quot;,\u0026quot;time\u0026quot;,t,\u0026quot;beta\u0026quot;,b,\u0026quot;gamma\u0026quot;,g,\u0026quot;.png\u0026quot;), width=8, height=6, dpi=180) The fully annotated code can be found in my GitHub repository SIR-interact. You can download the code and tinker around with it.\nSo when we run the function while setting the period to 70 days, β = 1.4 and γ = 0.3 SIR.model(70,1.4,0.3) we write a file with the name SIRplot_time70beta1.4gamma0.3.png and get output plot looking like…\n  ","date":1483488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586625311,"objectID":"876983ddfdf32412f156f5701d9b52a4","permalink":"/post/sir-model/","publishdate":"2017-01-04T00:00:00Z","relpermalink":"/post/sir-model/","section":"post","summary":"Original post date: January 04 2017\nIn my infectious disease epidemiology course back in 2016, I first learned about a basic compartmental disease transmission model. I thought it was pretty awesome to predict and model how an infectious agent could affect a population. Purely out of interest I wanted to develop some working code in R to make a function that would plot the model based on specified parameters.","tags":[],"title":"SIR model with deSolve and ggplot2","type":"post"},{"authors":["Eugene Joh","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math .\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580687852,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Eugene Joh","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math .\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580687852,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]